{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DF_GAN_all_in_one.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyBxzZ53Zx_3",
        "outputId": "81a72dad-9bd1-434b-a5b8-0a7b065e44ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Nov  7 14:20:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSTwg1N4NOEX"
      },
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwL9TxOp0zoj",
        "outputId": "f6b116bf-2699-4408-c6fa-0325bd4730c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/My Drive/text2image'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9Tnj0SvNOEe",
        "outputId": "e1d27074-5d0e-418d-8697-4f48400e6746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('data/embeddings', exist_ok=True)\n",
        "os.makedirs('/content/drive/My Drive/text2image/fake_images', exist_ok=True)\n",
        "os.makedirs('/content/drive/My Drive/text2image/models', exist_ok=True)\n",
        "if not os.path.exists('GDriveDL.py'):\n",
        "    !wget https://raw.githubusercontent.com/matthuisman/gdrivedl/master/gdrivedl.py -O GDriveDL.py\n",
        "\n",
        "\n",
        "if not os.path.exists('data/eda_ru.zip'): \n",
        "    !python GDriveDL.py https://drive.google.com/file/d/1CNIbj8_OuxQD74zt6JU4BUI8ngctutl9/view?usp=sharing data\n",
        "    !unzip -q data/eda_ru.zip -d data\n",
        "# if not os.path.exists('df_gan.zip'):        \n",
        "#     !python GDriveDL.py https://drive.google.com/open?id=1KKAqwSbHd-_qMpOAjYdbBRCh4M-HbRTH .\n",
        "#     !unzip -q df_gan.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-07 14:21:14--  https://raw.githubusercontent.com/matthuisman/gdrivedl/master/gdrivedl.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6773 (6.6K) [text/plain]\n",
            "Saving to: ‘GDriveDL.py’\n",
            "\n",
            "GDriveDL.py         100%[===================>]   6.61K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-07 14:21:14 (44.3 MB/s) - ‘GDriveDL.py’ saved [6773/6773]\n",
            "\n",
            "data/eda_ru.zip\n",
            "[==================================================] 1891.11MB/1891.11MB\n",
            "CPU times: user 10.4 s, sys: 2.8 s, total: 13.2 s\n",
            "Wall time: 2min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pDmIG_zNqdV"
      },
      "source": [
        "class NetG(nn.Module):\n",
        "    def __init__(self, ngf=64, nz=100):\n",
        "        super(NetG, self).__init__()\n",
        "\n",
        "        self.fc_embedding = nn.Linear(768, 256)\n",
        "\n",
        "        self.ngf = ngf\n",
        "\n",
        "        # layer1输入的是一个100x1x1的随机噪声, 输出尺寸(ngf*8)x4x4\n",
        "        self.fc = nn.Linear(nz, ngf*8*4*4)\n",
        "        self.block0 = G_Block(ngf * 8, ngf * 8)#4x4\n",
        "        self.block1 = G_Block(ngf * 8, ngf * 8)#4x4\n",
        "        self.block2 = G_Block(ngf * 8, ngf * 8)#8x8\n",
        "        self.block3 = G_Block(ngf * 8, ngf * 8)#16x16\n",
        "        self.block4 = G_Block(ngf * 8, ngf * 4)#32x32\n",
        "        self.block5 = G_Block(ngf * 4, ngf * 2)#64x64\n",
        "        self.block6 = G_Block(ngf * 2, ngf * 1)#128x128\n",
        "\n",
        "        self.conv_img = nn.Sequential(\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(ngf, 3, 3, 1, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "\n",
        "        c = self.fc_embedding(c)\n",
        "\n",
        "        out = self.fc(x)\n",
        "        out = out.view(x.size(0), 8*self.ngf, 4, 4)\n",
        "        out = self.block0(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block1(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block2(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block3(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block4(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block5(out,c)\n",
        "\n",
        "        out = F.interpolate(out, scale_factor=2)\n",
        "        out = self.block6(out,c)\n",
        "\n",
        "        out = self.conv_img(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class G_Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(G_Block, self).__init__()\n",
        "\n",
        "        self.learnable_sc = in_ch != out_ch \n",
        "        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
        "        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
        "        self.affine0 = affine(in_ch)\n",
        "        self.affine1 = affine(in_ch)\n",
        "        self.affine2 = affine(out_ch)\n",
        "        self.affine3 = affine(out_ch)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        if self.learnable_sc:\n",
        "            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        return self.shortcut(x) + self.gamma * self.residual(x, y)\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self.c_sc(x)\n",
        "        return x\n",
        "\n",
        "    def residual(self, x, y=None):\n",
        "        h = self.affine0(x, y)\n",
        "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
        "        h = self.affine1(h, y)\n",
        "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
        "        h = self.c1(h)\n",
        "        \n",
        "        h = self.affine2(h, y)\n",
        "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
        "        h = self.affine3(h, y)\n",
        "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
        "        return self.c2(h)\n",
        "\n",
        "\n",
        "\n",
        "class affine(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super(affine, self).__init__()\n",
        "\n",
        "        self.fc_gamma = nn.Sequential(OrderedDict([\n",
        "            ('linear1',nn.Linear(256, 256)),\n",
        "            ('relu1',nn.ReLU(inplace=True)),\n",
        "            ('linear2',nn.Linear(256, num_features)),\n",
        "            ]))\n",
        "        self.fc_beta = nn.Sequential(OrderedDict([\n",
        "            ('linear1',nn.Linear(256, 256)),\n",
        "            ('relu1',nn.ReLU(inplace=True)),\n",
        "            ('linear2',nn.Linear(256, num_features)),\n",
        "            ]))\n",
        "        self._initialize()\n",
        "\n",
        "    def _initialize(self):\n",
        "        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n",
        "        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n",
        "        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n",
        "        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "\n",
        "        weight = self.fc_gamma(y)\n",
        "        bias = self.fc_beta(y)        \n",
        "\n",
        "        if weight.dim() == 1:\n",
        "            weight = weight.unsqueeze(0)\n",
        "        if bias.dim() == 1:\n",
        "            bias = bias.unsqueeze(0)\n",
        "\n",
        "        size = x.size()\n",
        "        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
        "        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
        "        return weight * x + bias\n",
        "\n",
        "\n",
        "class D_GET_LOGITS(nn.Module):\n",
        "    def __init__(self, ndf):\n",
        "        super(D_GET_LOGITS, self).__init__()\n",
        "        self.df_dim = ndf\n",
        "        self.fc_embedding = nn.Linear(768, 256)\n",
        "\n",
        "        self.joint_conv = nn.Sequential(\n",
        "            nn.Conv2d(ndf * 16+256, ndf * 2, 3, 1, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2,inplace=True),\n",
        "            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, out, y):\n",
        "\n",
        "        y = self.fc_embedding(y)\n",
        "        \n",
        "        y = y.view(-1, 256, 1, 1)\n",
        "        y = y.repeat(1, 1, 4, 4)\n",
        "        h_c_code = torch.cat((out, y), 1)\n",
        "        out = self.joint_conv(h_c_code)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NetD(nn.Module):\n",
        "    def __init__(self, ndf):\n",
        "        super(NetD, self).__init__()\n",
        "\n",
        "        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)#128\n",
        "        self.block0 = resD(ndf * 1, ndf * 2)#64\n",
        "        self.block1 = resD(ndf * 2, ndf * 4)#32\n",
        "        self.block2 = resD(ndf * 4, ndf * 8)#16\n",
        "        self.block3 = resD(ndf * 8, ndf * 16)#8\n",
        "        self.block4 = resD(ndf * 16, ndf * 16)#4\n",
        "        self.block5 = resD(ndf * 16, ndf * 16)#4\n",
        "\n",
        "        self.COND_DNET = D_GET_LOGITS(ndf)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        out = self.conv_img(x)\n",
        "        out = self.block0(out)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.block5(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class resD(nn.Module):\n",
        "    def __init__(self, fin, fout, downsample=True):\n",
        "        super().__init__()\n",
        "        self.downsample = downsample\n",
        "        self.learned_shortcut = (fin != fout)\n",
        "        self.conv_r = nn.Sequential(\n",
        "            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.conv_s = nn.Conv2d(fin,fout, 1, stride=1, padding=0)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x, c=None):\n",
        "        return self.shortcut(x)+self.gamma*self.residual(x)\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learned_shortcut:\n",
        "            x = self.conv_s(x)\n",
        "        if self.downsample:\n",
        "            return F.avg_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "    def residual(self, x):\n",
        "        return self.conv_r(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaKUwBDoNOEl"
      },
      "source": [
        "### 1) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp0WqatPNOEm"
      },
      "source": [
        "class RecipeDataset(data.Dataset):\n",
        "    def __init__(self, \n",
        "                 data_dir='data', \n",
        "                 csv_filename='eda_ru_filtered.csv',\n",
        "                 use_last_image=False,\n",
        "                 base_size=64,\n",
        "                 transform=None, target_transform=None):\n",
        "        self.transform = transform\n",
        "        self.norm = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.target_transform = target_transform\n",
        "        \n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.csv_filename = csv_filename\n",
        "        self.use_last_image = use_last_image\n",
        "        \n",
        "        self.data = pd.read_csv(self.data_dir / self.csv_filename, usecols=['id', 'images'])\n",
        "        self.ids = self.data['id'].values\n",
        "        self.images = self.data['images'].values\n",
        "        \n",
        "        self.embeddings = self.load_embeddings()\n",
        "\n",
        "    \n",
        "    def load_embeddings(self):\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        for idx in self.ids:\n",
        "            title_emb = np.load(f'{self.data_dir}/embeddings/title/{idx}.npz')['arr_0']\n",
        "            # ingredients_emb = np.load(f'{self.data_dir}/embeddings/ingredients/{idx}.npz')['arr_0']\n",
        "            # steps_emb = np.load(f'{self.data_dir}/embeddings/steps/{idx}.npz')['arr_0']\n",
        "\n",
        "            embeddings.append(\n",
        "                np.concatenate([title_emb], axis=1)\n",
        "            )\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #\n",
        "        image_pathes = self.images[idx].split('|')\n",
        "        if self.use_last_image:\n",
        "            img_path = image_pathes[-1]\n",
        "        else:\n",
        "            img_path = random.choice(image_pathes)\n",
        "       \n",
        "        # Загружаем изображение\n",
        "        img = Image.open(self.data_dir / img_path).convert('RGB')\n",
        "        width, height = img.size\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        imgs = []\n",
        "        imgs.append(self.norm(img))\n",
        "        \n",
        "        # ИСПРАВИТЬ - временно для того чтобы проверить работоспособность в целом\n",
        "        emb = self.embeddings[idx, :][0]\n",
        "        \n",
        "        return imgs, emb\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5eWxLRrNOEp"
      },
      "source": [
        "# embeddings = 'RuBERT_mean_embeddings_long'\n",
        "# embeddings_type = 'whole_recipe'\n",
        "# train_data = RecipeDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ruvDXVNOEr"
      },
      "source": [
        "#### Посмотрим что получилось:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSylcA_DNOEr"
      },
      "source": [
        "# print(train_data.ids[:5])\n",
        "# print(train_data.recipes[:5])\n",
        "# print(train_data.filenames[:5])\n",
        "# print(len(train_data))\n",
        "# imshow(train_data[1][0][0].numpy().transpose(1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXn6sHUNOEw"
      },
      "source": [
        "## 2) Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uRiK_leNOEw",
        "outputId": "749ed70c-03ef-459a-bbed-e025b758dfff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed = 100\n",
        "print(\"seed now is : \", seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed now is :  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swNJBDvTNOE0",
        "outputId": "23bcceef-2b0b-450f-a234-abb26b1c4a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfOT0k0lNOE2"
      },
      "source": [
        "#### Config:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZbbtxOjNOE3"
      },
      "source": [
        "config = {}\n",
        "\n",
        "config['MODEL_NAME'] = 'eda_ru_tuned_bert_title_emb_only'\n",
        "\n",
        "config['CUDA'] = True\n",
        "config['WORKERS'] = 4\n",
        "\n",
        "config['loss'] = 'hinge'\n",
        "config['BASE_SIZE'] = 64\n",
        "\n",
        "# config['DATASET'] = {}\n",
        "# config['DATASET']['EMBEDDINGS_FILE'] = 'RuBERT_mean_embeddings_long'\n",
        "# config['DATASET']['EMBEDDINGS_TYPE'] = 'whole_recipe'\n",
        "\n",
        "\n",
        "config['TRAIN'] = {}\n",
        "config['TRAIN']['BATCH_SIZE'] = 16\n",
        "config['TRAIN']['MAX_EPOCH'] = 512\n",
        "config['TRAIN']['SNAPSHOT_INTERVAL'] = 2000\n",
        "config['TRAIN']['DISCRIMINATOR_LR'] = 2e-4\n",
        "config['TRAIN']['GENERATOR_LR'] = 2e-4\n",
        "config['TRAIN']['ENCODER_LR'] = 2e-4\n",
        "# config['TRAIN']['RNN_GRAD_CLIP'] = 0.25\n",
        "# config['TRAIN']['FLAG'] = True\n",
        "# config['TRAIN']['NET_E'] = ''\n",
        "# config['TRAIN']['NET_G'] = ''\n",
        "# config['TRAIN']['B_NET_D'] = True\n",
        "config['TRAIN']['NF'] = 32\n",
        "config['TRAIN']['SMOOTH'] = {}\n",
        "config['TRAIN']['SMOOTH']['GAMMA1'] = 5.0\n",
        "config['TRAIN']['SMOOTH']['GAMMA1'] = 10.0\n",
        "config['TRAIN']['SMOOTH']['GAMMA1'] = 5.0\n",
        "config['TRAIN']['SMOOTH']['GAMMA1'] = 1.0\n",
        "\n",
        "config['GAN'] = {}\n",
        "config['GAN']['DF_DIM'] = 64\n",
        "config['GAN']['GF_DIM'] = 128\n",
        "config['GAN']['Z_DIM'] = 100\n",
        "config['GAN']['CONDITION_DIM'] = 100\n",
        "config['GAN']['R_NUM'] = 2\n",
        "config['GAN']['B_ATTENTION'] = True\n",
        "config['GAN']['B_DCGAN'] = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwP8-C08NOE6",
        "outputId": "e8a29c89-da97-4863-af17-b077697ef7d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "imsize = 64\n",
        "\n",
        "# dataset and dataloader\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize(int(imsize * 76 / 64)),\n",
        "    transforms.RandomCrop(imsize),\n",
        "    transforms.RandomHorizontalFlip()])\n",
        "\n",
        "   \n",
        "dataset = RecipeDataset()\n",
        "\n",
        "print(len(dataset))\n",
        "assert dataset\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=config['TRAIN']['BATCH_SIZE'], drop_last=True,\n",
        "    shuffle=True, num_workers=config['WORKERS'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I79bOCvpNOE8"
      },
      "source": [
        "def prepare_data(data):\n",
        "    imgs, embds = data\n",
        "\n",
        "    real_imgs = []\n",
        "    for i in range(len(imgs)):\n",
        "        if config['CUDA']:\n",
        "            real_imgs.append(Variable(imgs[i]).cuda())\n",
        "        else:\n",
        "            real_imgs.append(Variable(imgs[i]))\n",
        "\n",
        "    if config['CUDA']:\n",
        "        embds = Variable(embds).cuda()\n",
        "    else:\n",
        "        embds = Variable(embds)\n",
        "\n",
        "    return [real_imgs, embds]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv0kIEAqNOE-"
      },
      "source": [
        "def train(dataloader, netG, netD, optimizerG, optimizerD, state_epoch, batch_size, device):\n",
        "    fake_images = Path('fake_images')\n",
        "    fake_images.mkdir(exist_ok=True)\n",
        "    for epoch in range(state_epoch + 1, config['TRAIN']['MAX_EPOCH'] + 1):\n",
        "        for step, data in enumerate(dataloader, 0):\n",
        "    \n",
        "            imags, sent_emb = prepare_data(data)\n",
        "#             hidden = text_encoder.init_hidden(batch_size)\n",
        "#             # words_embs: batch_size x nef x seq_len\n",
        "#             # sent_emb: batch_size x nef\n",
        "#             words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
        "#             words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
        "\n",
        "            imgs=imags[0].to(device)\n",
        "            real_features = netD(imgs)\n",
        "            output = netD.COND_DNET(real_features, sent_emb)\n",
        "            errD_real = torch.nn.ReLU()(1.0 - output).mean()\n",
        "\n",
        "            output = netD.COND_DNET(real_features[:(batch_size - 1)], sent_emb[1:batch_size])\n",
        "            errD_mismatch = torch.nn.ReLU()(1.0 + output).mean()\n",
        "\n",
        "            # synthesize fake images\n",
        "            noise = torch.randn(batch_size, 100)\n",
        "            noise=noise.to(device)\n",
        "            fake = netG(noise,sent_emb)  \n",
        "            \n",
        "            # G does not need update with D\n",
        "            fake_features = netD(fake.detach()) \n",
        "\n",
        "            errD_fake = netD.COND_DNET(fake_features,sent_emb)\n",
        "            errD_fake = torch.nn.ReLU()(1.0 + errD_fake).mean()          \n",
        "\n",
        "            errD = errD_real + (errD_fake + errD_mismatch)/2.0\n",
        "            optimizerD.zero_grad()\n",
        "            optimizerG.zero_grad()\n",
        "            errD.backward()\n",
        "            optimizerD.step()\n",
        "\n",
        "            #MA-GP\n",
        "            interpolated = (imgs.data).requires_grad_(True)\n",
        "            sent_inter = (sent_emb.data).requires_grad_(True)\n",
        "            features = netD(interpolated)\n",
        "            out = netD.COND_DNET(features,sent_inter)\n",
        "            grads = torch.autograd.grad(outputs=out,\n",
        "                                    inputs=(interpolated,sent_inter),\n",
        "                                    grad_outputs=torch.ones(out.size()).cuda(),\n",
        "                                    retain_graph=True,\n",
        "                                    create_graph=True,\n",
        "                                    only_inputs=True)\n",
        "            grad0 = grads[0].view(grads[0].size(0), -1)\n",
        "            grad1 = grads[1].view(grads[1].size(0), -1)\n",
        "            grad = torch.cat((grad0,grad1),dim=1)                        \n",
        "            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
        "            d_loss_gp = torch.mean((grad_l2norm) ** 6)\n",
        "            d_loss = 2.0 * d_loss_gp\n",
        "            optimizerD.zero_grad()\n",
        "            optimizerG.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizerD.step()\n",
        "            \n",
        "            # update G\n",
        "            features = netD(fake)\n",
        "            output = netD.COND_DNET(features,sent_emb)\n",
        "            errG = - output.mean()\n",
        "            optimizerG.zero_grad()\n",
        "            optimizerD.zero_grad()\n",
        "            errG.backward()\n",
        "            optimizerG.step()\n",
        "            if step % 50 == 0:\n",
        "                print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'\n",
        "                    % (epoch, config['TRAIN']['MAX_EPOCH'], step, len(dataloader), errD.item(), errG.item()))\n",
        "\n",
        "                path_image = f'{drive_path}/fake_images/{config[\"MODEL_NAME\"]}_{epoch}_{step}.png' \n",
        "                vutils.save_image(fake.data, path_image, normalize=True)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            path_netG = f'{drive_path}/models/{config[\"MODEL_NAME\"]}_netG_{epoch}.pth' \n",
        "            path_netD = f'{drive_path}/models/{config[\"MODEL_NAME\"]}_netD_{epoch}.pth' \n",
        "            torch.save(netG.state_dict(), path_netG)\n",
        "            torch.save(netD.state_dict(), path_netD)      "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40vbxeneNOFA"
      },
      "source": [
        "netG = NetG(config['TRAIN']['NF'], 100)\n",
        "netD = NetD(config['TRAIN']['NF'])\n",
        "state_epoch=0\n",
        "\n",
        "\n",
        "load_epoch = 81\n",
        "path_netG = f'{drive_path}/models/{config[\"MODEL_NAME\"]}_netG_{load_epoch}.pth' \n",
        "path_netD = f'{drive_path}/models/{config[\"MODEL_NAME\"]}_netD_{load_epoch}.pth' \n",
        "netG.load_state_dict(torch.load(path_netG))\n",
        "netD.load_state_dict(torch.load(path_netD))\n",
        "state_epoch = load_epoch\n",
        "\n",
        "\n",
        "netG.to(device)\n",
        "netD.to(device)\n",
        "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
        "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0004, betas=(0.0, 0.9))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NILpHmENOFC",
        "outputId": "91d61236-2409-43ce-d6f2-dd207072f2b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(dataloader, netG, netD, optimizerG, optimizerD, state_epoch, config['TRAIN']['BATCH_SIZE'], device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[82/512][0/1880] Loss_D: 1.253 Loss_G 0.720\n",
            "[82/512][50/1880] Loss_D: 1.369 Loss_G 0.555\n",
            "[82/512][100/1880] Loss_D: 1.403 Loss_G 0.699\n",
            "[82/512][150/1880] Loss_D: 1.431 Loss_G 1.651\n",
            "[82/512][200/1880] Loss_D: 1.330 Loss_G 2.010\n",
            "[82/512][250/1880] Loss_D: 1.570 Loss_G 1.717\n",
            "[82/512][300/1880] Loss_D: 1.541 Loss_G 1.933\n",
            "[82/512][350/1880] Loss_D: 1.452 Loss_G 0.739\n",
            "[82/512][400/1880] Loss_D: 1.319 Loss_G 1.224\n",
            "[82/512][450/1880] Loss_D: 1.310 Loss_G 1.949\n",
            "[82/512][500/1880] Loss_D: 1.590 Loss_G 1.707\n",
            "[82/512][550/1880] Loss_D: 1.426 Loss_G 1.804\n",
            "[82/512][600/1880] Loss_D: 1.327 Loss_G 1.227\n",
            "[82/512][650/1880] Loss_D: 1.774 Loss_G 1.373\n",
            "[82/512][700/1880] Loss_D: 1.676 Loss_G 1.274\n",
            "[82/512][750/1880] Loss_D: 1.375 Loss_G 2.035\n",
            "[82/512][800/1880] Loss_D: 1.238 Loss_G 1.000\n",
            "[82/512][850/1880] Loss_D: 1.312 Loss_G 1.612\n",
            "[82/512][900/1880] Loss_D: 1.376 Loss_G 1.530\n",
            "[82/512][950/1880] Loss_D: 1.443 Loss_G 0.985\n",
            "[82/512][1000/1880] Loss_D: 1.533 Loss_G 1.594\n",
            "[82/512][1050/1880] Loss_D: 1.324 Loss_G 1.276\n",
            "[82/512][1100/1880] Loss_D: 1.230 Loss_G 0.952\n",
            "[82/512][1150/1880] Loss_D: 1.461 Loss_G 0.116\n",
            "[82/512][1200/1880] Loss_D: 1.447 Loss_G 1.889\n",
            "[82/512][1250/1880] Loss_D: 1.578 Loss_G 1.660\n",
            "[82/512][1300/1880] Loss_D: 1.315 Loss_G 1.975\n",
            "[82/512][1350/1880] Loss_D: 1.366 Loss_G 0.970\n",
            "[82/512][1400/1880] Loss_D: 1.426 Loss_G 1.912\n",
            "[82/512][1450/1880] Loss_D: 1.558 Loss_G 1.268\n",
            "[82/512][1500/1880] Loss_D: 1.408 Loss_G 0.540\n",
            "[82/512][1550/1880] Loss_D: 1.291 Loss_G 0.312\n",
            "[82/512][1600/1880] Loss_D: 1.281 Loss_G 1.242\n",
            "[82/512][1650/1880] Loss_D: 1.577 Loss_G 1.107\n",
            "[82/512][1700/1880] Loss_D: 1.281 Loss_G 1.794\n",
            "[82/512][1750/1880] Loss_D: 1.418 Loss_G 1.680\n",
            "[82/512][1800/1880] Loss_D: 1.448 Loss_G 1.647\n",
            "[82/512][1850/1880] Loss_D: 1.461 Loss_G 0.632\n",
            "[83/512][0/1880] Loss_D: 1.440 Loss_G 0.494\n",
            "[83/512][50/1880] Loss_D: 1.333 Loss_G 1.487\n",
            "[83/512][100/1880] Loss_D: 1.573 Loss_G 2.109\n",
            "[83/512][150/1880] Loss_D: 1.633 Loss_G 1.443\n",
            "[83/512][200/1880] Loss_D: 1.399 Loss_G 2.671\n",
            "[83/512][250/1880] Loss_D: 1.404 Loss_G 1.870\n",
            "[83/512][300/1880] Loss_D: 1.372 Loss_G 0.810\n",
            "[83/512][350/1880] Loss_D: 1.182 Loss_G 0.847\n",
            "[83/512][400/1880] Loss_D: 1.681 Loss_G 1.500\n",
            "[83/512][450/1880] Loss_D: 1.352 Loss_G 0.515\n",
            "[83/512][500/1880] Loss_D: 1.469 Loss_G 0.237\n",
            "[83/512][550/1880] Loss_D: 1.335 Loss_G 1.459\n",
            "[83/512][600/1880] Loss_D: 1.510 Loss_G 1.543\n",
            "[83/512][650/1880] Loss_D: 1.605 Loss_G 1.492\n",
            "[83/512][700/1880] Loss_D: 1.528 Loss_G 0.273\n",
            "[83/512][750/1880] Loss_D: 1.360 Loss_G 1.004\n",
            "[83/512][800/1880] Loss_D: 1.523 Loss_G 0.515\n",
            "[83/512][850/1880] Loss_D: 1.265 Loss_G 1.126\n",
            "[83/512][900/1880] Loss_D: 1.513 Loss_G 0.609\n",
            "[83/512][950/1880] Loss_D: 1.378 Loss_G 0.915\n",
            "[83/512][1000/1880] Loss_D: 1.347 Loss_G 0.424\n",
            "[83/512][1050/1880] Loss_D: 1.408 Loss_G 1.241\n",
            "[83/512][1100/1880] Loss_D: 1.539 Loss_G 0.386\n",
            "[83/512][1150/1880] Loss_D: 1.334 Loss_G 1.545\n",
            "[83/512][1200/1880] Loss_D: 1.574 Loss_G 0.749\n",
            "[83/512][1250/1880] Loss_D: 1.426 Loss_G 0.428\n",
            "[83/512][1300/1880] Loss_D: 1.292 Loss_G 2.045\n",
            "[83/512][1350/1880] Loss_D: 1.291 Loss_G 1.518\n",
            "[83/512][1400/1880] Loss_D: 1.351 Loss_G 0.558\n",
            "[83/512][1450/1880] Loss_D: 1.318 Loss_G 1.035\n",
            "[83/512][1500/1880] Loss_D: 1.501 Loss_G 1.692\n",
            "[83/512][1550/1880] Loss_D: 1.418 Loss_G 1.584\n",
            "[83/512][1600/1880] Loss_D: 1.469 Loss_G 0.379\n",
            "[83/512][1650/1880] Loss_D: 1.418 Loss_G 1.453\n",
            "[83/512][1700/1880] Loss_D: 1.425 Loss_G 0.783\n",
            "[83/512][1750/1880] Loss_D: 1.491 Loss_G 0.397\n",
            "[83/512][1800/1880] Loss_D: 1.549 Loss_G 1.374\n",
            "[83/512][1850/1880] Loss_D: 1.644 Loss_G 1.329\n",
            "[84/512][0/1880] Loss_D: 1.287 Loss_G 1.424\n",
            "[84/512][50/1880] Loss_D: 1.601 Loss_G 1.425\n",
            "[84/512][100/1880] Loss_D: 1.432 Loss_G 0.039\n",
            "[84/512][150/1880] Loss_D: 1.413 Loss_G 1.787\n",
            "[84/512][200/1880] Loss_D: 1.299 Loss_G -0.004\n",
            "[84/512][250/1880] Loss_D: 1.269 Loss_G 1.868\n",
            "[84/512][300/1880] Loss_D: 1.431 Loss_G 1.674\n",
            "[84/512][350/1880] Loss_D: 1.224 Loss_G 1.226\n",
            "[84/512][400/1880] Loss_D: 1.280 Loss_G 1.573\n",
            "[84/512][450/1880] Loss_D: 1.343 Loss_G 1.756\n",
            "[84/512][500/1880] Loss_D: 1.660 Loss_G 1.531\n",
            "[84/512][550/1880] Loss_D: 1.246 Loss_G 1.996\n",
            "[84/512][600/1880] Loss_D: 1.537 Loss_G 0.358\n",
            "[84/512][650/1880] Loss_D: 1.413 Loss_G 1.045\n",
            "[84/512][700/1880] Loss_D: 1.349 Loss_G 0.871\n",
            "[84/512][750/1880] Loss_D: 1.238 Loss_G 2.144\n",
            "[84/512][800/1880] Loss_D: 1.359 Loss_G 2.241\n",
            "[84/512][850/1880] Loss_D: 1.436 Loss_G 1.490\n",
            "[84/512][900/1880] Loss_D: 1.423 Loss_G 0.275\n",
            "[84/512][950/1880] Loss_D: 1.247 Loss_G 1.049\n",
            "[84/512][1000/1880] Loss_D: 1.303 Loss_G 1.872\n",
            "[84/512][1050/1880] Loss_D: 1.765 Loss_G 0.989\n",
            "[84/512][1100/1880] Loss_D: 1.277 Loss_G 0.282\n",
            "[84/512][1150/1880] Loss_D: 1.318 Loss_G 0.868\n",
            "[84/512][1200/1880] Loss_D: 1.411 Loss_G 0.547\n",
            "[84/512][1250/1880] Loss_D: 1.465 Loss_G 1.418\n",
            "[84/512][1300/1880] Loss_D: 1.358 Loss_G 0.900\n",
            "[84/512][1350/1880] Loss_D: 1.479 Loss_G 0.381\n",
            "[84/512][1400/1880] Loss_D: 1.484 Loss_G 0.673\n",
            "[84/512][1450/1880] Loss_D: 1.368 Loss_G 1.938\n",
            "[84/512][1500/1880] Loss_D: 1.485 Loss_G 1.587\n",
            "[84/512][1550/1880] Loss_D: 1.425 Loss_G 1.634\n",
            "[84/512][1600/1880] Loss_D: 1.331 Loss_G 2.340\n",
            "[84/512][1650/1880] Loss_D: 1.385 Loss_G 1.785\n",
            "[84/512][1700/1880] Loss_D: 1.381 Loss_G 0.206\n",
            "[84/512][1750/1880] Loss_D: 1.358 Loss_G 1.806\n",
            "[84/512][1800/1880] Loss_D: 1.523 Loss_G 1.313\n",
            "[84/512][1850/1880] Loss_D: 1.292 Loss_G 0.677\n",
            "[85/512][0/1880] Loss_D: 1.361 Loss_G 1.654\n",
            "[85/512][50/1880] Loss_D: 1.354 Loss_G 0.827\n",
            "[85/512][100/1880] Loss_D: 1.384 Loss_G 0.112\n",
            "[85/512][150/1880] Loss_D: 1.196 Loss_G 0.361\n",
            "[85/512][200/1880] Loss_D: 1.588 Loss_G 0.503\n",
            "[85/512][250/1880] Loss_D: 1.296 Loss_G 0.999\n",
            "[85/512][300/1880] Loss_D: 1.521 Loss_G 1.640\n",
            "[85/512][350/1880] Loss_D: 1.474 Loss_G 1.910\n",
            "[85/512][400/1880] Loss_D: 1.381 Loss_G 1.367\n",
            "[85/512][450/1880] Loss_D: 1.625 Loss_G 1.698\n",
            "[85/512][500/1880] Loss_D: 1.320 Loss_G 0.777\n",
            "[85/512][550/1880] Loss_D: 1.392 Loss_G 0.512\n",
            "[85/512][600/1880] Loss_D: 1.335 Loss_G 0.434\n",
            "[85/512][650/1880] Loss_D: 1.381 Loss_G 1.726\n",
            "[85/512][700/1880] Loss_D: 1.429 Loss_G 2.131\n",
            "[85/512][750/1880] Loss_D: 1.359 Loss_G 1.394\n",
            "[85/512][800/1880] Loss_D: 1.506 Loss_G 0.314\n",
            "[85/512][850/1880] Loss_D: 1.265 Loss_G 1.894\n",
            "[85/512][900/1880] Loss_D: 1.603 Loss_G 2.427\n",
            "[85/512][950/1880] Loss_D: 1.306 Loss_G 1.837\n",
            "[85/512][1000/1880] Loss_D: 1.541 Loss_G 2.072\n",
            "[85/512][1050/1880] Loss_D: 1.355 Loss_G 2.227\n",
            "[85/512][1100/1880] Loss_D: 1.677 Loss_G 1.397\n",
            "[85/512][1150/1880] Loss_D: 1.278 Loss_G 1.645\n",
            "[85/512][1200/1880] Loss_D: 1.465 Loss_G 2.021\n",
            "[85/512][1250/1880] Loss_D: 1.271 Loss_G 1.655\n",
            "[85/512][1300/1880] Loss_D: 1.469 Loss_G 1.917\n",
            "[85/512][1350/1880] Loss_D: 1.362 Loss_G 0.561\n",
            "[85/512][1400/1880] Loss_D: 1.382 Loss_G 0.503\n",
            "[85/512][1450/1880] Loss_D: 1.418 Loss_G 0.374\n",
            "[85/512][1500/1880] Loss_D: 1.575 Loss_G 1.569\n",
            "[85/512][1550/1880] Loss_D: 1.733 Loss_G 1.072\n",
            "[85/512][1600/1880] Loss_D: 1.451 Loss_G 1.841\n",
            "[85/512][1650/1880] Loss_D: 1.608 Loss_G 0.301\n",
            "[85/512][1700/1880] Loss_D: 1.287 Loss_G 0.820\n",
            "[85/512][1750/1880] Loss_D: 1.357 Loss_G 0.441\n",
            "[85/512][1800/1880] Loss_D: 1.265 Loss_G 0.569\n",
            "[85/512][1850/1880] Loss_D: 1.395 Loss_G 2.160\n",
            "[86/512][0/1880] Loss_D: 1.236 Loss_G 0.720\n",
            "[86/512][50/1880] Loss_D: 1.409 Loss_G 0.617\n",
            "[86/512][100/1880] Loss_D: 1.575 Loss_G 1.001\n",
            "[86/512][150/1880] Loss_D: 1.230 Loss_G 1.203\n",
            "[86/512][200/1880] Loss_D: 1.367 Loss_G 0.344\n",
            "[86/512][250/1880] Loss_D: 1.421 Loss_G 0.246\n",
            "[86/512][300/1880] Loss_D: 1.515 Loss_G 1.440\n",
            "[86/512][350/1880] Loss_D: 1.550 Loss_G 2.076\n",
            "[86/512][400/1880] Loss_D: 1.530 Loss_G 1.332\n",
            "[86/512][450/1880] Loss_D: 1.633 Loss_G 1.380\n",
            "[86/512][500/1880] Loss_D: 1.264 Loss_G 0.713\n",
            "[86/512][550/1880] Loss_D: 1.405 Loss_G 1.776\n",
            "[86/512][600/1880] Loss_D: 1.405 Loss_G 1.285\n",
            "[86/512][650/1880] Loss_D: 1.254 Loss_G 1.740\n",
            "[86/512][700/1880] Loss_D: 1.623 Loss_G 1.128\n",
            "[86/512][750/1880] Loss_D: 1.612 Loss_G 1.843\n",
            "[86/512][800/1880] Loss_D: 1.326 Loss_G 1.714\n",
            "[86/512][850/1880] Loss_D: 1.419 Loss_G 0.981\n",
            "[86/512][900/1880] Loss_D: 1.187 Loss_G 1.654\n",
            "[86/512][950/1880] Loss_D: 1.479 Loss_G 0.180\n",
            "[86/512][1000/1880] Loss_D: 1.409 Loss_G 0.925\n",
            "[86/512][1050/1880] Loss_D: 1.334 Loss_G 0.892\n",
            "[86/512][1100/1880] Loss_D: 1.749 Loss_G 0.704\n",
            "[86/512][1150/1880] Loss_D: 1.684 Loss_G 1.264\n",
            "[86/512][1200/1880] Loss_D: 1.352 Loss_G 0.909\n",
            "[86/512][1250/1880] Loss_D: 1.508 Loss_G 1.855\n",
            "[86/512][1300/1880] Loss_D: 1.428 Loss_G 0.380\n",
            "[86/512][1350/1880] Loss_D: 1.691 Loss_G 1.234\n",
            "[86/512][1400/1880] Loss_D: 1.347 Loss_G 0.622\n",
            "[86/512][1450/1880] Loss_D: 1.445 Loss_G 2.030\n",
            "[86/512][1500/1880] Loss_D: 1.327 Loss_G 0.747\n",
            "[86/512][1550/1880] Loss_D: 1.342 Loss_G 0.292\n",
            "[86/512][1600/1880] Loss_D: 1.532 Loss_G 1.817\n",
            "[86/512][1650/1880] Loss_D: 1.312 Loss_G 1.524\n",
            "[86/512][1700/1880] Loss_D: 1.368 Loss_G 1.733\n",
            "[86/512][1750/1880] Loss_D: 1.402 Loss_G 0.229\n",
            "[86/512][1800/1880] Loss_D: 1.404 Loss_G 0.414\n",
            "[86/512][1850/1880] Loss_D: 1.363 Loss_G 1.920\n",
            "[87/512][0/1880] Loss_D: 1.549 Loss_G 1.390\n",
            "[87/512][50/1880] Loss_D: 1.486 Loss_G 1.415\n",
            "[87/512][100/1880] Loss_D: 1.320 Loss_G 1.524\n",
            "[87/512][150/1880] Loss_D: 1.333 Loss_G 1.045\n",
            "[87/512][200/1880] Loss_D: 1.429 Loss_G 0.656\n",
            "[87/512][250/1880] Loss_D: 1.348 Loss_G 0.825\n",
            "[87/512][300/1880] Loss_D: 1.201 Loss_G 0.958\n",
            "[87/512][350/1880] Loss_D: 1.428 Loss_G 2.271\n",
            "[87/512][400/1880] Loss_D: 1.455 Loss_G 1.935\n",
            "[87/512][450/1880] Loss_D: 1.581 Loss_G 1.817\n",
            "[87/512][500/1880] Loss_D: 1.649 Loss_G 2.033\n",
            "[87/512][550/1880] Loss_D: 1.397 Loss_G 1.961\n",
            "[87/512][600/1880] Loss_D: 1.374 Loss_G 1.909\n",
            "[87/512][650/1880] Loss_D: 1.333 Loss_G 1.698\n",
            "[87/512][700/1880] Loss_D: 1.545 Loss_G 0.367\n",
            "[87/512][750/1880] Loss_D: 1.362 Loss_G 1.495\n",
            "[87/512][800/1880] Loss_D: 1.487 Loss_G 2.060\n",
            "[87/512][850/1880] Loss_D: 1.455 Loss_G 0.028\n",
            "[87/512][900/1880] Loss_D: 1.414 Loss_G 0.376\n",
            "[87/512][950/1880] Loss_D: 1.374 Loss_G 1.366\n",
            "[87/512][1000/1880] Loss_D: 1.184 Loss_G 1.642\n",
            "[87/512][1050/1880] Loss_D: 1.318 Loss_G 2.236\n",
            "[87/512][1100/1880] Loss_D: 1.587 Loss_G 1.472\n",
            "[87/512][1150/1880] Loss_D: 1.119 Loss_G 1.528\n",
            "[87/512][1200/1880] Loss_D: 1.721 Loss_G 1.071\n",
            "[87/512][1250/1880] Loss_D: 1.224 Loss_G 1.950\n",
            "[87/512][1300/1880] Loss_D: 1.369 Loss_G 1.628\n",
            "[87/512][1350/1880] Loss_D: 1.550 Loss_G 1.765\n",
            "[87/512][1400/1880] Loss_D: 1.510 Loss_G 1.801\n",
            "[87/512][1450/1880] Loss_D: 1.543 Loss_G 0.250\n",
            "[87/512][1500/1880] Loss_D: 1.352 Loss_G 1.834\n",
            "[87/512][1550/1880] Loss_D: 1.294 Loss_G 1.244\n",
            "[87/512][1600/1880] Loss_D: 1.787 Loss_G 1.785\n",
            "[87/512][1650/1880] Loss_D: 1.323 Loss_G 1.034\n",
            "[87/512][1700/1880] Loss_D: 1.209 Loss_G 0.939\n",
            "[87/512][1750/1880] Loss_D: 1.353 Loss_G 1.935\n",
            "[87/512][1800/1880] Loss_D: 1.296 Loss_G 1.870\n",
            "[87/512][1850/1880] Loss_D: 1.388 Loss_G 1.547\n",
            "[88/512][0/1880] Loss_D: 1.358 Loss_G 2.112\n",
            "[88/512][50/1880] Loss_D: 1.374 Loss_G 1.769\n",
            "[88/512][100/1880] Loss_D: 1.294 Loss_G 1.752\n",
            "[88/512][150/1880] Loss_D: 1.226 Loss_G 0.846\n",
            "[88/512][200/1880] Loss_D: 1.329 Loss_G 1.076\n",
            "[88/512][250/1880] Loss_D: 1.580 Loss_G 0.237\n",
            "[88/512][300/1880] Loss_D: 1.442 Loss_G 0.408\n",
            "[88/512][350/1880] Loss_D: 1.571 Loss_G 1.578\n",
            "[88/512][400/1880] Loss_D: 1.289 Loss_G 0.142\n",
            "[88/512][450/1880] Loss_D: 1.366 Loss_G 2.399\n",
            "[88/512][500/1880] Loss_D: 1.413 Loss_G 0.633\n",
            "[88/512][550/1880] Loss_D: 1.419 Loss_G 0.526\n",
            "[88/512][600/1880] Loss_D: 1.486 Loss_G 1.774\n",
            "[88/512][650/1880] Loss_D: 1.320 Loss_G 1.649\n",
            "[88/512][700/1880] Loss_D: 1.375 Loss_G 1.616\n",
            "[88/512][750/1880] Loss_D: 1.431 Loss_G 1.451\n",
            "[88/512][800/1880] Loss_D: 1.313 Loss_G 1.845\n",
            "[88/512][850/1880] Loss_D: 1.400 Loss_G 1.446\n",
            "[88/512][900/1880] Loss_D: 1.410 Loss_G 1.615\n",
            "[88/512][950/1880] Loss_D: 1.605 Loss_G 1.173\n",
            "[88/512][1000/1880] Loss_D: 1.513 Loss_G 2.002\n",
            "[88/512][1050/1880] Loss_D: 1.455 Loss_G 0.876\n",
            "[88/512][1100/1880] Loss_D: 1.345 Loss_G 2.157\n",
            "[88/512][1150/1880] Loss_D: 1.397 Loss_G 0.985\n",
            "[88/512][1200/1880] Loss_D: 1.212 Loss_G 0.738\n",
            "[88/512][1250/1880] Loss_D: 1.513 Loss_G 1.905\n",
            "[88/512][1300/1880] Loss_D: 1.544 Loss_G 1.531\n",
            "[88/512][1350/1880] Loss_D: 1.296 Loss_G 0.526\n",
            "[88/512][1400/1880] Loss_D: 1.422 Loss_G 2.340\n",
            "[88/512][1450/1880] Loss_D: 1.414 Loss_G 0.135\n",
            "[88/512][1500/1880] Loss_D: 1.251 Loss_G 2.238\n",
            "[88/512][1550/1880] Loss_D: 1.296 Loss_G 0.867\n",
            "[88/512][1600/1880] Loss_D: 1.597 Loss_G 1.894\n",
            "[88/512][1650/1880] Loss_D: 1.375 Loss_G 0.298\n",
            "[88/512][1700/1880] Loss_D: 1.266 Loss_G 0.487\n",
            "[88/512][1750/1880] Loss_D: 1.263 Loss_G 1.238\n",
            "[88/512][1800/1880] Loss_D: 1.395 Loss_G 1.682\n",
            "[88/512][1850/1880] Loss_D: 1.278 Loss_G 1.783\n",
            "[89/512][0/1880] Loss_D: 1.329 Loss_G 1.370\n",
            "[89/512][50/1880] Loss_D: 1.364 Loss_G 2.208\n",
            "[89/512][100/1880] Loss_D: 1.422 Loss_G 1.253\n",
            "[89/512][150/1880] Loss_D: 1.434 Loss_G 0.141\n",
            "[89/512][200/1880] Loss_D: 1.279 Loss_G 0.669\n",
            "[89/512][250/1880] Loss_D: 1.372 Loss_G 0.789\n",
            "[89/512][300/1880] Loss_D: 1.351 Loss_G 1.693\n",
            "[89/512][350/1880] Loss_D: 1.339 Loss_G 1.826\n",
            "[89/512][400/1880] Loss_D: 1.494 Loss_G 2.090\n",
            "[89/512][450/1880] Loss_D: 1.286 Loss_G 0.948\n",
            "[89/512][500/1880] Loss_D: 1.361 Loss_G 0.753\n",
            "[89/512][550/1880] Loss_D: 1.365 Loss_G 1.529\n",
            "[89/512][600/1880] Loss_D: 1.342 Loss_G 2.151\n",
            "[89/512][650/1880] Loss_D: 1.287 Loss_G 0.794\n",
            "[89/512][700/1880] Loss_D: 1.345 Loss_G 2.399\n",
            "[89/512][750/1880] Loss_D: 1.622 Loss_G 2.273\n",
            "[89/512][800/1880] Loss_D: 1.428 Loss_G 1.953\n",
            "[89/512][850/1880] Loss_D: 1.472 Loss_G 2.045\n",
            "[89/512][900/1880] Loss_D: 1.410 Loss_G 0.476\n",
            "[89/512][950/1880] Loss_D: 1.489 Loss_G 1.812\n",
            "[89/512][1000/1880] Loss_D: 1.502 Loss_G 1.940\n",
            "[89/512][1050/1880] Loss_D: 1.307 Loss_G 1.327\n",
            "[89/512][1100/1880] Loss_D: 1.479 Loss_G 2.016\n",
            "[89/512][1150/1880] Loss_D: 1.475 Loss_G 0.676\n",
            "[89/512][1200/1880] Loss_D: 1.406 Loss_G 1.027\n",
            "[89/512][1250/1880] Loss_D: 1.538 Loss_G -0.102\n",
            "[89/512][1300/1880] Loss_D: 1.256 Loss_G 0.669\n",
            "[89/512][1350/1880] Loss_D: 1.497 Loss_G 0.571\n",
            "[89/512][1400/1880] Loss_D: 1.854 Loss_G 1.074\n",
            "[89/512][1450/1880] Loss_D: 1.358 Loss_G 1.669\n",
            "[89/512][1500/1880] Loss_D: 1.716 Loss_G 1.262\n",
            "[89/512][1550/1880] Loss_D: 1.293 Loss_G 2.029\n",
            "[89/512][1600/1880] Loss_D: 1.469 Loss_G 0.698\n",
            "[89/512][1650/1880] Loss_D: 1.299 Loss_G 1.184\n",
            "[89/512][1700/1880] Loss_D: 1.404 Loss_G 0.708\n",
            "[89/512][1750/1880] Loss_D: 1.625 Loss_G 1.261\n",
            "[89/512][1800/1880] Loss_D: 1.382 Loss_G 1.968\n",
            "[89/512][1850/1880] Loss_D: 1.263 Loss_G 1.893\n",
            "[90/512][0/1880] Loss_D: 1.270 Loss_G 1.980\n",
            "[90/512][50/1880] Loss_D: 1.579 Loss_G 1.568\n",
            "[90/512][100/1880] Loss_D: 1.380 Loss_G 0.720\n",
            "[90/512][150/1880] Loss_D: 1.188 Loss_G 0.787\n",
            "[90/512][200/1880] Loss_D: 1.405 Loss_G 1.109\n",
            "[90/512][250/1880] Loss_D: 1.547 Loss_G 0.291\n",
            "[90/512][300/1880] Loss_D: 1.651 Loss_G 2.007\n",
            "[90/512][350/1880] Loss_D: 1.633 Loss_G -0.179\n",
            "[90/512][400/1880] Loss_D: 1.344 Loss_G 0.252\n",
            "[90/512][450/1880] Loss_D: 1.656 Loss_G 1.704\n",
            "[90/512][500/1880] Loss_D: 1.251 Loss_G 1.256\n",
            "[90/512][550/1880] Loss_D: 1.463 Loss_G 0.170\n",
            "[90/512][600/1880] Loss_D: 1.531 Loss_G 1.881\n",
            "[90/512][650/1880] Loss_D: 1.599 Loss_G 0.369\n",
            "[90/512][700/1880] Loss_D: 1.275 Loss_G 0.984\n",
            "[90/512][750/1880] Loss_D: 1.336 Loss_G 1.870\n",
            "[90/512][800/1880] Loss_D: 1.369 Loss_G 0.523\n",
            "[90/512][850/1880] Loss_D: 1.254 Loss_G 1.620\n",
            "[90/512][900/1880] Loss_D: 1.496 Loss_G 1.903\n",
            "[90/512][950/1880] Loss_D: 1.305 Loss_G 0.654\n",
            "[90/512][1000/1880] Loss_D: 1.366 Loss_G 0.736\n",
            "[90/512][1050/1880] Loss_D: 1.453 Loss_G -0.015\n",
            "[90/512][1100/1880] Loss_D: 1.390 Loss_G 0.869\n",
            "[90/512][1150/1880] Loss_D: 1.293 Loss_G -0.167\n",
            "[90/512][1200/1880] Loss_D: 1.291 Loss_G 0.738\n",
            "[90/512][1250/1880] Loss_D: 1.268 Loss_G 0.832\n",
            "[90/512][1300/1880] Loss_D: 1.564 Loss_G 1.928\n",
            "[90/512][1350/1880] Loss_D: 1.403 Loss_G 1.812\n",
            "[90/512][1400/1880] Loss_D: 1.396 Loss_G 1.971\n",
            "[90/512][1450/1880] Loss_D: 1.382 Loss_G 1.742\n",
            "[90/512][1500/1880] Loss_D: 1.398 Loss_G 1.915\n",
            "[90/512][1550/1880] Loss_D: 1.399 Loss_G 1.916\n",
            "[90/512][1600/1880] Loss_D: 1.455 Loss_G 0.531\n",
            "[90/512][1650/1880] Loss_D: 1.294 Loss_G 1.772\n",
            "[90/512][1700/1880] Loss_D: 1.295 Loss_G 1.097\n",
            "[90/512][1750/1880] Loss_D: 1.531 Loss_G 1.879\n",
            "[90/512][1800/1880] Loss_D: 1.393 Loss_G 0.974\n",
            "[90/512][1850/1880] Loss_D: 1.297 Loss_G 1.030\n",
            "[91/512][0/1880] Loss_D: 1.307 Loss_G 0.799\n",
            "[91/512][50/1880] Loss_D: 1.542 Loss_G 1.698\n",
            "[91/512][100/1880] Loss_D: 1.393 Loss_G 1.549\n",
            "[91/512][150/1880] Loss_D: 1.320 Loss_G 0.451\n",
            "[91/512][200/1880] Loss_D: 1.276 Loss_G 0.794\n",
            "[91/512][250/1880] Loss_D: 1.564 Loss_G 1.336\n",
            "[91/512][300/1880] Loss_D: 1.474 Loss_G 1.926\n",
            "[91/512][350/1880] Loss_D: 1.348 Loss_G 1.987\n",
            "[91/512][400/1880] Loss_D: 1.370 Loss_G 0.502\n",
            "[91/512][450/1880] Loss_D: 1.286 Loss_G 2.262\n",
            "[91/512][500/1880] Loss_D: 1.423 Loss_G 1.399\n",
            "[91/512][550/1880] Loss_D: 1.350 Loss_G -0.010\n",
            "[91/512][600/1880] Loss_D: 1.388 Loss_G 1.165\n",
            "[91/512][650/1880] Loss_D: 1.450 Loss_G 0.636\n",
            "[91/512][700/1880] Loss_D: 1.359 Loss_G 1.374\n",
            "[91/512][750/1880] Loss_D: 1.394 Loss_G 1.009\n",
            "[91/512][800/1880] Loss_D: 1.707 Loss_G 1.254\n",
            "[91/512][850/1880] Loss_D: 1.372 Loss_G 1.801\n",
            "[91/512][900/1880] Loss_D: 1.511 Loss_G 1.910\n",
            "[91/512][950/1880] Loss_D: 1.255 Loss_G 1.415\n",
            "[91/512][1000/1880] Loss_D: 1.419 Loss_G 2.316\n",
            "[91/512][1050/1880] Loss_D: 1.429 Loss_G 2.309\n",
            "[91/512][1100/1880] Loss_D: 1.405 Loss_G 0.579\n",
            "[91/512][1150/1880] Loss_D: 1.348 Loss_G 1.759\n",
            "[91/512][1200/1880] Loss_D: 1.471 Loss_G 2.972\n",
            "[91/512][1250/1880] Loss_D: 1.292 Loss_G 1.451\n",
            "[91/512][1300/1880] Loss_D: 1.317 Loss_G 1.889\n",
            "[91/512][1350/1880] Loss_D: 1.384 Loss_G 1.847\n",
            "[91/512][1400/1880] Loss_D: 1.280 Loss_G 1.215\n",
            "[91/512][1450/1880] Loss_D: 1.447 Loss_G 1.505\n",
            "[91/512][1500/1880] Loss_D: 1.431 Loss_G 0.358\n",
            "[91/512][1550/1880] Loss_D: 1.443 Loss_G 0.127\n",
            "[91/512][1600/1880] Loss_D: 1.309 Loss_G 1.402\n",
            "[91/512][1650/1880] Loss_D: 1.555 Loss_G 0.443\n",
            "[91/512][1700/1880] Loss_D: 1.317 Loss_G 1.478\n",
            "[91/512][1750/1880] Loss_D: 1.398 Loss_G 1.138\n",
            "[91/512][1800/1880] Loss_D: 1.613 Loss_G 1.278\n",
            "[91/512][1850/1880] Loss_D: 1.318 Loss_G 2.192\n",
            "[92/512][0/1880] Loss_D: 1.237 Loss_G 0.314\n",
            "[92/512][50/1880] Loss_D: 1.295 Loss_G 1.350\n",
            "[92/512][100/1880] Loss_D: 1.113 Loss_G 0.831\n",
            "[92/512][150/1880] Loss_D: 1.286 Loss_G 1.233\n",
            "[92/512][200/1880] Loss_D: 1.603 Loss_G 0.018\n",
            "[92/512][250/1880] Loss_D: 1.496 Loss_G 2.028\n",
            "[92/512][300/1880] Loss_D: 1.496 Loss_G 0.177\n",
            "[92/512][350/1880] Loss_D: 1.437 Loss_G 0.612\n",
            "[92/512][400/1880] Loss_D: 1.364 Loss_G 0.327\n",
            "[92/512][450/1880] Loss_D: 1.322 Loss_G 0.807\n",
            "[92/512][500/1880] Loss_D: 1.415 Loss_G 0.115\n",
            "[92/512][550/1880] Loss_D: 1.318 Loss_G 2.376\n",
            "[92/512][600/1880] Loss_D: 1.690 Loss_G 1.765\n",
            "[92/512][650/1880] Loss_D: 1.143 Loss_G 0.452\n",
            "[92/512][700/1880] Loss_D: 1.505 Loss_G 1.997\n",
            "[92/512][750/1880] Loss_D: 1.464 Loss_G 0.133\n",
            "[92/512][800/1880] Loss_D: 1.311 Loss_G 1.166\n",
            "[92/512][850/1880] Loss_D: 1.340 Loss_G 1.243\n",
            "[92/512][900/1880] Loss_D: 1.733 Loss_G -0.741\n",
            "[92/512][950/1880] Loss_D: 1.311 Loss_G 0.305\n",
            "[92/512][1000/1880] Loss_D: 1.297 Loss_G 0.653\n",
            "[92/512][1050/1880] Loss_D: 1.262 Loss_G 1.324\n",
            "[92/512][1100/1880] Loss_D: 1.252 Loss_G 0.585\n",
            "[92/512][1150/1880] Loss_D: 1.323 Loss_G 0.277\n",
            "[92/512][1200/1880] Loss_D: 1.345 Loss_G 0.312\n",
            "[92/512][1250/1880] Loss_D: 1.519 Loss_G 1.525\n",
            "[92/512][1300/1880] Loss_D: 1.693 Loss_G -0.011\n",
            "[92/512][1350/1880] Loss_D: 1.596 Loss_G 1.314\n",
            "[92/512][1400/1880] Loss_D: 1.623 Loss_G 1.528\n",
            "[92/512][1450/1880] Loss_D: 1.318 Loss_G 1.983\n",
            "[92/512][1500/1880] Loss_D: 1.393 Loss_G 0.691\n",
            "[92/512][1550/1880] Loss_D: 1.357 Loss_G 2.063\n",
            "[92/512][1600/1880] Loss_D: 1.269 Loss_G 0.867\n",
            "[92/512][1650/1880] Loss_D: 1.245 Loss_G 1.889\n",
            "[92/512][1700/1880] Loss_D: 1.338 Loss_G 0.322\n",
            "[92/512][1750/1880] Loss_D: 1.416 Loss_G 1.821\n",
            "[92/512][1800/1880] Loss_D: 1.533 Loss_G 1.950\n",
            "[92/512][1850/1880] Loss_D: 1.290 Loss_G 1.042\n",
            "[93/512][0/1880] Loss_D: 1.368 Loss_G 2.066\n",
            "[93/512][50/1880] Loss_D: 1.428 Loss_G 0.570\n",
            "[93/512][100/1880] Loss_D: 1.377 Loss_G 1.721\n",
            "[93/512][150/1880] Loss_D: 1.439 Loss_G 0.461\n",
            "[93/512][200/1880] Loss_D: 1.420 Loss_G 2.114\n",
            "[93/512][250/1880] Loss_D: 1.506 Loss_G 0.395\n",
            "[93/512][300/1880] Loss_D: 1.412 Loss_G 1.468\n",
            "[93/512][350/1880] Loss_D: 1.334 Loss_G 1.979\n",
            "[93/512][400/1880] Loss_D: 1.303 Loss_G 0.940\n",
            "[93/512][450/1880] Loss_D: 1.598 Loss_G 1.700\n",
            "[93/512][500/1880] Loss_D: 1.289 Loss_G 2.361\n",
            "[93/512][550/1880] Loss_D: 1.523 Loss_G 2.006\n",
            "[93/512][600/1880] Loss_D: 1.332 Loss_G -0.031\n",
            "[93/512][650/1880] Loss_D: 1.134 Loss_G 0.332\n",
            "[93/512][700/1880] Loss_D: 1.227 Loss_G 1.369\n",
            "[93/512][750/1880] Loss_D: 1.598 Loss_G 1.268\n",
            "[93/512][800/1880] Loss_D: 1.619 Loss_G 1.135\n",
            "[93/512][850/1880] Loss_D: 1.225 Loss_G 0.876\n",
            "[93/512][900/1880] Loss_D: 1.494 Loss_G 0.433\n",
            "[93/512][950/1880] Loss_D: 1.226 Loss_G 1.630\n",
            "[93/512][1000/1880] Loss_D: 1.362 Loss_G 0.441\n",
            "[93/512][1050/1880] Loss_D: 1.746 Loss_G 1.260\n",
            "[93/512][1100/1880] Loss_D: 1.406 Loss_G 1.273\n",
            "[93/512][1150/1880] Loss_D: 1.673 Loss_G 1.093\n",
            "[93/512][1200/1880] Loss_D: 1.482 Loss_G 1.458\n",
            "[93/512][1250/1880] Loss_D: 1.549 Loss_G 0.310\n",
            "[93/512][1300/1880] Loss_D: 1.326 Loss_G 2.085\n",
            "[93/512][1350/1880] Loss_D: 1.469 Loss_G 1.886\n",
            "[93/512][1400/1880] Loss_D: 1.508 Loss_G 0.351\n",
            "[93/512][1450/1880] Loss_D: 1.340 Loss_G 1.859\n",
            "[93/512][1500/1880] Loss_D: 1.363 Loss_G 1.525\n",
            "[93/512][1550/1880] Loss_D: 1.217 Loss_G 0.892\n",
            "[93/512][1600/1880] Loss_D: 1.436 Loss_G -0.226\n",
            "[93/512][1650/1880] Loss_D: 1.217 Loss_G 2.191\n",
            "[93/512][1700/1880] Loss_D: 1.454 Loss_G 0.407\n",
            "[93/512][1750/1880] Loss_D: 1.401 Loss_G 0.158\n",
            "[93/512][1800/1880] Loss_D: 1.601 Loss_G 1.556\n",
            "[93/512][1850/1880] Loss_D: 1.383 Loss_G 0.747\n",
            "[94/512][0/1880] Loss_D: 1.402 Loss_G 1.400\n",
            "[94/512][50/1880] Loss_D: 1.340 Loss_G 0.947\n",
            "[94/512][100/1880] Loss_D: 1.451 Loss_G 0.141\n",
            "[94/512][150/1880] Loss_D: 1.378 Loss_G 2.164\n",
            "[94/512][200/1880] Loss_D: 1.567 Loss_G 1.851\n",
            "[94/512][250/1880] Loss_D: 1.240 Loss_G 1.691\n",
            "[94/512][300/1880] Loss_D: 1.330 Loss_G 0.271\n",
            "[94/512][350/1880] Loss_D: 1.719 Loss_G 1.045\n",
            "[94/512][400/1880] Loss_D: 1.317 Loss_G 1.397\n",
            "[94/512][450/1880] Loss_D: 1.222 Loss_G 0.103\n",
            "[94/512][500/1880] Loss_D: 1.490 Loss_G 1.904\n",
            "[94/512][550/1880] Loss_D: 1.328 Loss_G 0.591\n",
            "[94/512][600/1880] Loss_D: 1.264 Loss_G 1.904\n",
            "[94/512][650/1880] Loss_D: 1.433 Loss_G 2.103\n",
            "[94/512][700/1880] Loss_D: 1.362 Loss_G 0.767\n",
            "[94/512][750/1880] Loss_D: 1.329 Loss_G 1.149\n",
            "[94/512][800/1880] Loss_D: 1.455 Loss_G 1.864\n",
            "[94/512][850/1880] Loss_D: 1.509 Loss_G 1.730\n",
            "[94/512][900/1880] Loss_D: 1.465 Loss_G 0.752\n",
            "[94/512][950/1880] Loss_D: 1.207 Loss_G 1.893\n",
            "[94/512][1000/1880] Loss_D: 1.402 Loss_G 0.560\n",
            "[94/512][1050/1880] Loss_D: 1.493 Loss_G 1.803\n",
            "[94/512][1100/1880] Loss_D: 1.228 Loss_G 0.766\n",
            "[94/512][1150/1880] Loss_D: 1.323 Loss_G 1.224\n",
            "[94/512][1200/1880] Loss_D: 1.637 Loss_G 1.276\n",
            "[94/512][1250/1880] Loss_D: 1.294 Loss_G 0.644\n",
            "[94/512][1300/1880] Loss_D: 1.349 Loss_G 2.110\n",
            "[94/512][1350/1880] Loss_D: 1.472 Loss_G 1.722\n",
            "[94/512][1400/1880] Loss_D: 1.436 Loss_G 0.089\n",
            "[94/512][1450/1880] Loss_D: 1.450 Loss_G 0.303\n",
            "[94/512][1500/1880] Loss_D: 1.350 Loss_G 1.608\n",
            "[94/512][1550/1880] Loss_D: 1.649 Loss_G 2.307\n",
            "[94/512][1600/1880] Loss_D: 1.471 Loss_G 0.666\n",
            "[94/512][1650/1880] Loss_D: 1.523 Loss_G 1.265\n",
            "[94/512][1700/1880] Loss_D: 1.486 Loss_G 2.043\n",
            "[94/512][1750/1880] Loss_D: 1.393 Loss_G -0.396\n",
            "[94/512][1800/1880] Loss_D: 1.485 Loss_G 1.699\n",
            "[94/512][1850/1880] Loss_D: 1.214 Loss_G 1.596\n",
            "[95/512][0/1880] Loss_D: 1.197 Loss_G 2.243\n",
            "[95/512][50/1880] Loss_D: 1.153 Loss_G 1.149\n",
            "[95/512][100/1880] Loss_D: 1.344 Loss_G 0.587\n",
            "[95/512][150/1880] Loss_D: 1.410 Loss_G 1.781\n",
            "[95/512][200/1880] Loss_D: 1.431 Loss_G 1.567\n",
            "[95/512][250/1880] Loss_D: 1.361 Loss_G 1.650\n",
            "[95/512][300/1880] Loss_D: 1.509 Loss_G 1.458\n",
            "[95/512][350/1880] Loss_D: 1.395 Loss_G 1.997\n",
            "[95/512][400/1880] Loss_D: 1.375 Loss_G 1.575\n",
            "[95/512][450/1880] Loss_D: 1.370 Loss_G 2.080\n",
            "[95/512][500/1880] Loss_D: 1.318 Loss_G 1.299\n",
            "[95/512][550/1880] Loss_D: 1.420 Loss_G 1.762\n",
            "[95/512][600/1880] Loss_D: 1.239 Loss_G 0.525\n",
            "[95/512][650/1880] Loss_D: 1.333 Loss_G -0.103\n",
            "[95/512][700/1880] Loss_D: 1.765 Loss_G 1.684\n",
            "[95/512][750/1880] Loss_D: 1.233 Loss_G 1.852\n",
            "[95/512][800/1880] Loss_D: 1.282 Loss_G 1.138\n",
            "[95/512][850/1880] Loss_D: 1.900 Loss_G 1.457\n",
            "[95/512][900/1880] Loss_D: 1.460 Loss_G 1.355\n",
            "[95/512][950/1880] Loss_D: 1.490 Loss_G 2.382\n",
            "[95/512][1000/1880] Loss_D: 1.299 Loss_G 1.526\n",
            "[95/512][1050/1880] Loss_D: 1.354 Loss_G 0.448\n",
            "[95/512][1100/1880] Loss_D: 1.610 Loss_G -0.010\n",
            "[95/512][1150/1880] Loss_D: 1.302 Loss_G 2.305\n",
            "[95/512][1200/1880] Loss_D: 1.381 Loss_G 0.639\n",
            "[95/512][1250/1880] Loss_D: 1.416 Loss_G 1.970\n",
            "[95/512][1300/1880] Loss_D: 1.516 Loss_G 1.899\n",
            "[95/512][1350/1880] Loss_D: 1.422 Loss_G 1.895\n",
            "[95/512][1400/1880] Loss_D: 1.554 Loss_G 0.327\n",
            "[95/512][1450/1880] Loss_D: 1.287 Loss_G 0.591\n",
            "[95/512][1500/1880] Loss_D: 1.368 Loss_G 0.051\n",
            "[95/512][1550/1880] Loss_D: 1.554 Loss_G 2.266\n",
            "[95/512][1600/1880] Loss_D: 1.351 Loss_G 0.959\n",
            "[95/512][1650/1880] Loss_D: 1.462 Loss_G 0.666\n",
            "[95/512][1700/1880] Loss_D: 1.556 Loss_G 0.278\n",
            "[95/512][1750/1880] Loss_D: 1.360 Loss_G 1.435\n",
            "[95/512][1800/1880] Loss_D: 1.539 Loss_G 1.099\n",
            "[95/512][1850/1880] Loss_D: 1.261 Loss_G 1.735\n",
            "[96/512][0/1880] Loss_D: 1.175 Loss_G 2.231\n",
            "[96/512][50/1880] Loss_D: 1.368 Loss_G 0.474\n",
            "[96/512][100/1880] Loss_D: 1.457 Loss_G 2.333\n",
            "[96/512][150/1880] Loss_D: 1.166 Loss_G 2.484\n",
            "[96/512][200/1880] Loss_D: 1.515 Loss_G 1.855\n",
            "[96/512][250/1880] Loss_D: 1.333 Loss_G 1.971\n",
            "[96/512][300/1880] Loss_D: 1.463 Loss_G 0.465\n",
            "[96/512][350/1880] Loss_D: 1.323 Loss_G 0.465\n",
            "[96/512][400/1880] Loss_D: 1.508 Loss_G 1.628\n",
            "[96/512][450/1880] Loss_D: 1.240 Loss_G 0.955\n",
            "[96/512][500/1880] Loss_D: 1.437 Loss_G 0.518\n",
            "[96/512][550/1880] Loss_D: 1.336 Loss_G 0.516\n",
            "[96/512][600/1880] Loss_D: 1.508 Loss_G 1.545\n",
            "[96/512][650/1880] Loss_D: 1.328 Loss_G 0.044\n",
            "[96/512][700/1880] Loss_D: 1.271 Loss_G 2.181\n",
            "[96/512][750/1880] Loss_D: 1.532 Loss_G 1.978\n",
            "[96/512][800/1880] Loss_D: 1.465 Loss_G 1.776\n",
            "[96/512][850/1880] Loss_D: 1.478 Loss_G 1.510\n",
            "[96/512][900/1880] Loss_D: 1.399 Loss_G 1.317\n",
            "[96/512][950/1880] Loss_D: 1.491 Loss_G -0.069\n",
            "[96/512][1000/1880] Loss_D: 1.576 Loss_G 0.686\n",
            "[96/512][1050/1880] Loss_D: 1.541 Loss_G 2.031\n",
            "[96/512][1100/1880] Loss_D: 1.430 Loss_G 1.049\n",
            "[96/512][1150/1880] Loss_D: 1.373 Loss_G 0.639\n",
            "[96/512][1200/1880] Loss_D: 1.623 Loss_G 1.483\n",
            "[96/512][1250/1880] Loss_D: 1.719 Loss_G 1.248\n",
            "[96/512][1300/1880] Loss_D: 1.579 Loss_G 1.735\n",
            "[96/512][1350/1880] Loss_D: 1.626 Loss_G 1.301\n",
            "[96/512][1400/1880] Loss_D: 1.248 Loss_G 1.119\n",
            "[96/512][1450/1880] Loss_D: 1.688 Loss_G 1.505\n",
            "[96/512][1500/1880] Loss_D: 1.647 Loss_G 0.429\n",
            "[96/512][1550/1880] Loss_D: 1.320 Loss_G 1.828\n",
            "[96/512][1600/1880] Loss_D: 1.545 Loss_G 1.784\n",
            "[96/512][1650/1880] Loss_D: 1.279 Loss_G 1.199\n",
            "[96/512][1700/1880] Loss_D: 1.622 Loss_G 1.519\n",
            "[96/512][1750/1880] Loss_D: 1.378 Loss_G 0.658\n",
            "[96/512][1800/1880] Loss_D: 1.366 Loss_G 2.228\n",
            "[96/512][1850/1880] Loss_D: 1.445 Loss_G 1.554\n",
            "[97/512][0/1880] Loss_D: 1.366 Loss_G 2.375\n",
            "[97/512][50/1880] Loss_D: 1.483 Loss_G 1.932\n",
            "[97/512][100/1880] Loss_D: 1.233 Loss_G 2.094\n",
            "[97/512][150/1880] Loss_D: 1.373 Loss_G 1.942\n",
            "[97/512][200/1880] Loss_D: 1.729 Loss_G 1.353\n",
            "[97/512][250/1880] Loss_D: 1.473 Loss_G 1.597\n",
            "[97/512][300/1880] Loss_D: 1.368 Loss_G 0.552\n",
            "[97/512][350/1880] Loss_D: 1.200 Loss_G 1.822\n",
            "[97/512][400/1880] Loss_D: 1.314 Loss_G 1.806\n",
            "[97/512][450/1880] Loss_D: 1.272 Loss_G 1.194\n",
            "[97/512][500/1880] Loss_D: 1.338 Loss_G 1.885\n",
            "[97/512][550/1880] Loss_D: 1.784 Loss_G 1.233\n",
            "[97/512][600/1880] Loss_D: 1.392 Loss_G 1.065\n",
            "[97/512][650/1880] Loss_D: 1.446 Loss_G 0.454\n",
            "[97/512][700/1880] Loss_D: 1.735 Loss_G 1.840\n",
            "[97/512][750/1880] Loss_D: 1.382 Loss_G 0.407\n",
            "[97/512][800/1880] Loss_D: 1.327 Loss_G 2.093\n",
            "[97/512][850/1880] Loss_D: 1.347 Loss_G 1.967\n",
            "[97/512][900/1880] Loss_D: 1.372 Loss_G 1.573\n",
            "[97/512][950/1880] Loss_D: 1.362 Loss_G 0.658\n",
            "[97/512][1000/1880] Loss_D: 1.677 Loss_G 1.880\n",
            "[97/512][1050/1880] Loss_D: 1.242 Loss_G 1.400\n",
            "[97/512][1100/1880] Loss_D: 1.592 Loss_G 1.813\n",
            "[97/512][1150/1880] Loss_D: 1.367 Loss_G 2.135\n",
            "[97/512][1200/1880] Loss_D: 1.476 Loss_G 0.076\n",
            "[97/512][1250/1880] Loss_D: 1.437 Loss_G 0.579\n",
            "[97/512][1300/1880] Loss_D: 1.507 Loss_G 1.884\n",
            "[97/512][1350/1880] Loss_D: 1.464 Loss_G 0.592\n",
            "[97/512][1400/1880] Loss_D: 1.486 Loss_G 1.930\n",
            "[97/512][1450/1880] Loss_D: 1.298 Loss_G 2.642\n",
            "[97/512][1500/1880] Loss_D: 1.470 Loss_G 1.194\n",
            "[97/512][1550/1880] Loss_D: 1.264 Loss_G 0.949\n",
            "[97/512][1600/1880] Loss_D: 1.356 Loss_G 1.726\n",
            "[97/512][1650/1880] Loss_D: 1.318 Loss_G 2.094\n",
            "[97/512][1700/1880] Loss_D: 1.251 Loss_G 0.551\n",
            "[97/512][1750/1880] Loss_D: 1.615 Loss_G 1.492\n",
            "[97/512][1800/1880] Loss_D: 1.348 Loss_G 1.571\n",
            "[97/512][1850/1880] Loss_D: 1.322 Loss_G 1.890\n",
            "[98/512][0/1880] Loss_D: 1.305 Loss_G 0.322\n",
            "[98/512][50/1880] Loss_D: 1.288 Loss_G 0.500\n",
            "[98/512][100/1880] Loss_D: 1.409 Loss_G 1.879\n",
            "[98/512][150/1880] Loss_D: 1.466 Loss_G 1.807\n",
            "[98/512][200/1880] Loss_D: 1.259 Loss_G 0.552\n",
            "[98/512][250/1880] Loss_D: 1.478 Loss_G 0.523\n",
            "[98/512][300/1880] Loss_D: 1.555 Loss_G 1.970\n",
            "[98/512][350/1880] Loss_D: 1.076 Loss_G 0.887\n",
            "[98/512][400/1880] Loss_D: 1.364 Loss_G 0.546\n",
            "[98/512][450/1880] Loss_D: 1.339 Loss_G 1.901\n",
            "[98/512][500/1880] Loss_D: 1.305 Loss_G 1.871\n",
            "[98/512][550/1880] Loss_D: 1.347 Loss_G 1.992\n",
            "[98/512][600/1880] Loss_D: 1.378 Loss_G 0.129\n",
            "[98/512][650/1880] Loss_D: 1.247 Loss_G 0.873\n",
            "[98/512][700/1880] Loss_D: 1.338 Loss_G 2.076\n",
            "[98/512][750/1880] Loss_D: 1.365 Loss_G 1.902\n",
            "[98/512][800/1880] Loss_D: 1.372 Loss_G 0.677\n",
            "[98/512][850/1880] Loss_D: 1.375 Loss_G 2.149\n",
            "[98/512][900/1880] Loss_D: 1.450 Loss_G 2.040\n",
            "[98/512][950/1880] Loss_D: 1.465 Loss_G 1.155\n",
            "[98/512][1000/1880] Loss_D: 1.271 Loss_G 0.330\n",
            "[98/512][1050/1880] Loss_D: 1.492 Loss_G -0.189\n",
            "[98/512][1100/1880] Loss_D: 1.310 Loss_G 1.590\n",
            "[98/512][1150/1880] Loss_D: 1.603 Loss_G 1.571\n",
            "[98/512][1200/1880] Loss_D: 1.409 Loss_G 0.905\n",
            "[98/512][1250/1880] Loss_D: 1.503 Loss_G 1.773\n",
            "[98/512][1300/1880] Loss_D: 1.333 Loss_G 1.659\n",
            "[98/512][1350/1880] Loss_D: 1.357 Loss_G -0.012\n",
            "[98/512][1400/1880] Loss_D: 1.454 Loss_G -0.127\n",
            "[98/512][1450/1880] Loss_D: 1.408 Loss_G 1.886\n",
            "[98/512][1500/1880] Loss_D: 1.414 Loss_G 0.987\n",
            "[98/512][1550/1880] Loss_D: 1.288 Loss_G 0.703\n",
            "[98/512][1600/1880] Loss_D: 1.284 Loss_G 1.973\n",
            "[98/512][1650/1880] Loss_D: 1.368 Loss_G 0.730\n",
            "[98/512][1700/1880] Loss_D: 1.294 Loss_G 0.895\n",
            "[98/512][1750/1880] Loss_D: 1.431 Loss_G 0.066\n",
            "[98/512][1800/1880] Loss_D: 1.356 Loss_G 1.320\n",
            "[98/512][1850/1880] Loss_D: 1.274 Loss_G 0.706\n",
            "[99/512][0/1880] Loss_D: 1.330 Loss_G 1.670\n",
            "[99/512][50/1880] Loss_D: 1.294 Loss_G 1.733\n",
            "[99/512][100/1880] Loss_D: 1.565 Loss_G 0.308\n",
            "[99/512][150/1880] Loss_D: 1.309 Loss_G 0.495\n",
            "[99/512][200/1880] Loss_D: 1.340 Loss_G 1.459\n",
            "[99/512][250/1880] Loss_D: 1.387 Loss_G 0.739\n",
            "[99/512][300/1880] Loss_D: 1.596 Loss_G 1.559\n",
            "[99/512][350/1880] Loss_D: 1.203 Loss_G 1.275\n",
            "[99/512][400/1880] Loss_D: 1.620 Loss_G 1.022\n",
            "[99/512][450/1880] Loss_D: 1.619 Loss_G 1.962\n",
            "[99/512][500/1880] Loss_D: 1.277 Loss_G 1.988\n",
            "[99/512][550/1880] Loss_D: 1.486 Loss_G 0.215\n",
            "[99/512][600/1880] Loss_D: 1.502 Loss_G 1.970\n",
            "[99/512][650/1880] Loss_D: 1.381 Loss_G 2.103\n",
            "[99/512][700/1880] Loss_D: 1.539 Loss_G 1.489\n",
            "[99/512][750/1880] Loss_D: 1.416 Loss_G 1.736\n",
            "[99/512][800/1880] Loss_D: 1.452 Loss_G 0.215\n",
            "[99/512][850/1880] Loss_D: 1.387 Loss_G 0.532\n",
            "[99/512][900/1880] Loss_D: 1.468 Loss_G 0.846\n",
            "[99/512][950/1880] Loss_D: 1.545 Loss_G 0.472\n",
            "[99/512][1000/1880] Loss_D: 1.512 Loss_G 2.158\n",
            "[99/512][1050/1880] Loss_D: 1.386 Loss_G 1.632\n",
            "[99/512][1100/1880] Loss_D: 1.271 Loss_G 1.813\n",
            "[99/512][1150/1880] Loss_D: 1.262 Loss_G 0.765\n",
            "[99/512][1200/1880] Loss_D: 1.503 Loss_G 1.499\n",
            "[99/512][1250/1880] Loss_D: 1.476 Loss_G 1.802\n",
            "[99/512][1300/1880] Loss_D: 1.318 Loss_G -0.025\n",
            "[99/512][1350/1880] Loss_D: 1.368 Loss_G 2.062\n",
            "[99/512][1400/1880] Loss_D: 1.326 Loss_G 1.439\n",
            "[99/512][1450/1880] Loss_D: 1.442 Loss_G 1.144\n",
            "[99/512][1500/1880] Loss_D: 1.428 Loss_G 1.848\n",
            "[99/512][1550/1880] Loss_D: 1.534 Loss_G 1.451\n",
            "[99/512][1600/1880] Loss_D: 1.303 Loss_G 1.531\n",
            "[99/512][1650/1880] Loss_D: 1.452 Loss_G 0.496\n",
            "[99/512][1700/1880] Loss_D: 1.446 Loss_G 1.820\n",
            "[99/512][1750/1880] Loss_D: 1.332 Loss_G 1.488\n",
            "[99/512][1800/1880] Loss_D: 1.357 Loss_G 1.670\n",
            "[99/512][1850/1880] Loss_D: 1.445 Loss_G 0.246\n",
            "[100/512][0/1880] Loss_D: 1.372 Loss_G 0.206\n",
            "[100/512][50/1880] Loss_D: 1.323 Loss_G 2.423\n",
            "[100/512][100/1880] Loss_D: 1.320 Loss_G 0.494\n",
            "[100/512][150/1880] Loss_D: 1.180 Loss_G 0.635\n",
            "[100/512][200/1880] Loss_D: 1.491 Loss_G 0.516\n",
            "[100/512][250/1880] Loss_D: 1.290 Loss_G 0.527\n",
            "[100/512][300/1880] Loss_D: 1.360 Loss_G 0.854\n",
            "[100/512][350/1880] Loss_D: 1.447 Loss_G 0.394\n",
            "[100/512][400/1880] Loss_D: 1.309 Loss_G 1.840\n",
            "[100/512][450/1880] Loss_D: 1.432 Loss_G 0.152\n",
            "[100/512][500/1880] Loss_D: 1.364 Loss_G 1.643\n",
            "[100/512][550/1880] Loss_D: 1.422 Loss_G 0.330\n",
            "[100/512][600/1880] Loss_D: 1.405 Loss_G 0.322\n",
            "[100/512][650/1880] Loss_D: 1.291 Loss_G 0.418\n",
            "[100/512][700/1880] Loss_D: 1.414 Loss_G 0.553\n",
            "[100/512][750/1880] Loss_D: 1.322 Loss_G 1.885\n",
            "[100/512][800/1880] Loss_D: 1.463 Loss_G 2.506\n",
            "[100/512][850/1880] Loss_D: 1.429 Loss_G 0.221\n",
            "[100/512][900/1880] Loss_D: 1.548 Loss_G 0.094\n",
            "[100/512][950/1880] Loss_D: 1.583 Loss_G 1.658\n",
            "[100/512][1000/1880] Loss_D: 1.349 Loss_G 0.891\n",
            "[100/512][1050/1880] Loss_D: 1.207 Loss_G 1.949\n",
            "[100/512][1100/1880] Loss_D: 1.345 Loss_G 0.904\n",
            "[100/512][1150/1880] Loss_D: 1.582 Loss_G 1.313\n",
            "[100/512][1200/1880] Loss_D: 1.411 Loss_G 1.984\n",
            "[100/512][1250/1880] Loss_D: 1.634 Loss_G 1.248\n",
            "[100/512][1300/1880] Loss_D: 1.354 Loss_G 0.195\n",
            "[100/512][1350/1880] Loss_D: 1.355 Loss_G 0.391\n",
            "[100/512][1400/1880] Loss_D: 1.228 Loss_G 0.889\n",
            "[100/512][1450/1880] Loss_D: 1.534 Loss_G 0.331\n",
            "[100/512][1500/1880] Loss_D: 1.384 Loss_G 1.380\n",
            "[100/512][1550/1880] Loss_D: 1.442 Loss_G 1.650\n",
            "[100/512][1600/1880] Loss_D: 1.697 Loss_G 1.636\n",
            "[100/512][1650/1880] Loss_D: 1.472 Loss_G 0.759\n",
            "[100/512][1700/1880] Loss_D: 1.455 Loss_G 2.234\n",
            "[100/512][1750/1880] Loss_D: 1.278 Loss_G 2.012\n",
            "[100/512][1800/1880] Loss_D: 1.251 Loss_G 1.787\n",
            "[100/512][1850/1880] Loss_D: 1.302 Loss_G 1.034\n",
            "[101/512][0/1880] Loss_D: 1.274 Loss_G 0.305\n",
            "[101/512][50/1880] Loss_D: 1.215 Loss_G 0.499\n",
            "[101/512][100/1880] Loss_D: 1.282 Loss_G 1.972\n",
            "[101/512][150/1880] Loss_D: 1.358 Loss_G 0.609\n",
            "[101/512][200/1880] Loss_D: 1.312 Loss_G 2.026\n",
            "[101/512][250/1880] Loss_D: 1.333 Loss_G 0.899\n",
            "[101/512][300/1880] Loss_D: 1.422 Loss_G 0.119\n",
            "[101/512][350/1880] Loss_D: 1.269 Loss_G 1.742\n",
            "[101/512][400/1880] Loss_D: 1.305 Loss_G 0.564\n",
            "[101/512][450/1880] Loss_D: 1.321 Loss_G 1.963\n",
            "[101/512][500/1880] Loss_D: 1.278 Loss_G 1.576\n",
            "[101/512][550/1880] Loss_D: 1.372 Loss_G 1.933\n",
            "[101/512][600/1880] Loss_D: 1.309 Loss_G 0.676\n",
            "[101/512][650/1880] Loss_D: 1.175 Loss_G 1.136\n",
            "[101/512][700/1880] Loss_D: 1.532 Loss_G 0.444\n",
            "[101/512][750/1880] Loss_D: 1.732 Loss_G 1.925\n",
            "[101/512][800/1880] Loss_D: 1.249 Loss_G 2.051\n",
            "[101/512][850/1880] Loss_D: 1.214 Loss_G 1.557\n",
            "[101/512][900/1880] Loss_D: 1.296 Loss_G 2.203\n",
            "[101/512][950/1880] Loss_D: 1.295 Loss_G 1.176\n",
            "[101/512][1000/1880] Loss_D: 1.528 Loss_G 1.487\n",
            "[101/512][1050/1880] Loss_D: 1.337 Loss_G 0.390\n",
            "[101/512][1100/1880] Loss_D: 1.240 Loss_G 1.699\n",
            "[101/512][1150/1880] Loss_D: 1.390 Loss_G 2.070\n",
            "[101/512][1200/1880] Loss_D: 1.417 Loss_G 0.537\n",
            "[101/512][1250/1880] Loss_D: 1.615 Loss_G 1.846\n",
            "[101/512][1300/1880] Loss_D: 1.319 Loss_G 1.202\n",
            "[101/512][1350/1880] Loss_D: 1.152 Loss_G 1.803\n",
            "[101/512][1400/1880] Loss_D: 1.331 Loss_G 1.022\n",
            "[101/512][1450/1880] Loss_D: 1.324 Loss_G 0.506\n",
            "[101/512][1500/1880] Loss_D: 1.451 Loss_G 0.514\n",
            "[101/512][1550/1880] Loss_D: 1.478 Loss_G 1.151\n",
            "[101/512][1600/1880] Loss_D: 1.371 Loss_G 2.362\n",
            "[101/512][1650/1880] Loss_D: 1.330 Loss_G 1.410\n",
            "[101/512][1700/1880] Loss_D: 1.304 Loss_G 1.046\n",
            "[101/512][1750/1880] Loss_D: 1.388 Loss_G 1.566\n",
            "[101/512][1800/1880] Loss_D: 1.397 Loss_G 0.666\n",
            "[101/512][1850/1880] Loss_D: 1.449 Loss_G 1.583\n",
            "[102/512][0/1880] Loss_D: 1.352 Loss_G 0.910\n",
            "[102/512][50/1880] Loss_D: 1.199 Loss_G 0.041\n",
            "[102/512][100/1880] Loss_D: 1.253 Loss_G 0.754\n",
            "[102/512][150/1880] Loss_D: 1.324 Loss_G 1.565\n",
            "[102/512][200/1880] Loss_D: 1.303 Loss_G 1.094\n",
            "[102/512][250/1880] Loss_D: 1.309 Loss_G 0.360\n",
            "[102/512][300/1880] Loss_D: 1.353 Loss_G 2.138\n",
            "[102/512][350/1880] Loss_D: 1.352 Loss_G 0.706\n",
            "[102/512][400/1880] Loss_D: 1.239 Loss_G 1.341\n",
            "[102/512][450/1880] Loss_D: 1.453 Loss_G 1.704\n",
            "[102/512][500/1880] Loss_D: 1.376 Loss_G 2.216\n",
            "[102/512][550/1880] Loss_D: 1.666 Loss_G 0.800\n",
            "[102/512][600/1880] Loss_D: 1.290 Loss_G 1.816\n",
            "[102/512][650/1880] Loss_D: 1.422 Loss_G 0.241\n",
            "[102/512][700/1880] Loss_D: 1.273 Loss_G 1.586\n",
            "[102/512][750/1880] Loss_D: 1.326 Loss_G 1.827\n",
            "[102/512][800/1880] Loss_D: 1.375 Loss_G 1.622\n",
            "[102/512][850/1880] Loss_D: 1.444 Loss_G 1.873\n",
            "[102/512][900/1880] Loss_D: 1.471 Loss_G 1.571\n",
            "[102/512][950/1880] Loss_D: 1.398 Loss_G 1.889\n",
            "[102/512][1000/1880] Loss_D: 1.323 Loss_G 1.466\n",
            "[102/512][1050/1880] Loss_D: 1.371 Loss_G 1.596\n",
            "[102/512][1100/1880] Loss_D: 1.510 Loss_G 1.860\n",
            "[102/512][1150/1880] Loss_D: 1.196 Loss_G 2.065\n",
            "[102/512][1200/1880] Loss_D: 1.418 Loss_G 0.516\n",
            "[102/512][1250/1880] Loss_D: 1.274 Loss_G 0.544\n",
            "[102/512][1300/1880] Loss_D: 1.347 Loss_G 0.509\n",
            "[102/512][1350/1880] Loss_D: 1.475 Loss_G 0.025\n",
            "[102/512][1400/1880] Loss_D: 1.431 Loss_G -0.036\n",
            "[102/512][1450/1880] Loss_D: 1.913 Loss_G 0.774\n",
            "[102/512][1500/1880] Loss_D: 1.474 Loss_G 0.974\n",
            "[102/512][1550/1880] Loss_D: 1.322 Loss_G 0.044\n",
            "[102/512][1600/1880] Loss_D: 1.303 Loss_G 2.127\n",
            "[102/512][1650/1880] Loss_D: 1.383 Loss_G 0.334\n",
            "[102/512][1700/1880] Loss_D: 1.382 Loss_G 1.502\n",
            "[102/512][1750/1880] Loss_D: 1.286 Loss_G 0.124\n",
            "[102/512][1800/1880] Loss_D: 1.423 Loss_G 1.456\n",
            "[102/512][1850/1880] Loss_D: 1.555 Loss_G 0.625\n",
            "[103/512][0/1880] Loss_D: 1.369 Loss_G 1.520\n",
            "[103/512][50/1880] Loss_D: 1.201 Loss_G 1.654\n",
            "[103/512][100/1880] Loss_D: 1.485 Loss_G 1.630\n",
            "[103/512][150/1880] Loss_D: 1.335 Loss_G 1.183\n",
            "[103/512][200/1880] Loss_D: 1.616 Loss_G 1.386\n",
            "[103/512][250/1880] Loss_D: 1.330 Loss_G 0.238\n",
            "[103/512][300/1880] Loss_D: 1.412 Loss_G 1.669\n",
            "[103/512][350/1880] Loss_D: 1.218 Loss_G 0.506\n",
            "[103/512][400/1880] Loss_D: 1.536 Loss_G 1.803\n",
            "[103/512][450/1880] Loss_D: 1.348 Loss_G 1.171\n",
            "[103/512][500/1880] Loss_D: 1.473 Loss_G 0.480\n",
            "[103/512][550/1880] Loss_D: 1.421 Loss_G 1.873\n",
            "[103/512][600/1880] Loss_D: 1.239 Loss_G 0.912\n",
            "[103/512][650/1880] Loss_D: 1.349 Loss_G 2.405\n",
            "[103/512][700/1880] Loss_D: 1.523 Loss_G 2.041\n",
            "[103/512][750/1880] Loss_D: 1.323 Loss_G 2.260\n",
            "[103/512][800/1880] Loss_D: 1.596 Loss_G 1.642\n",
            "[103/512][850/1880] Loss_D: 1.537 Loss_G 0.136\n",
            "[103/512][900/1880] Loss_D: 1.410 Loss_G 1.837\n",
            "[103/512][950/1880] Loss_D: 1.377 Loss_G 0.962\n",
            "[103/512][1000/1880] Loss_D: 1.395 Loss_G 0.179\n",
            "[103/512][1050/1880] Loss_D: 1.683 Loss_G 1.356\n",
            "[103/512][1100/1880] Loss_D: 1.230 Loss_G 0.215\n",
            "[103/512][1150/1880] Loss_D: 1.207 Loss_G 1.657\n",
            "[103/512][1200/1880] Loss_D: 1.368 Loss_G 1.425\n",
            "[103/512][1250/1880] Loss_D: 1.623 Loss_G 0.542\n",
            "[103/512][1300/1880] Loss_D: 1.464 Loss_G 0.912\n",
            "[103/512][1350/1880] Loss_D: 1.244 Loss_G 1.318\n",
            "[103/512][1400/1880] Loss_D: 1.377 Loss_G 1.899\n",
            "[103/512][1450/1880] Loss_D: 1.412 Loss_G 1.834\n",
            "[103/512][1500/1880] Loss_D: 1.326 Loss_G 0.906\n",
            "[103/512][1550/1880] Loss_D: 1.269 Loss_G 1.502\n",
            "[103/512][1600/1880] Loss_D: 1.387 Loss_G 0.352\n",
            "[103/512][1650/1880] Loss_D: 1.488 Loss_G 0.844\n",
            "[103/512][1700/1880] Loss_D: 1.345 Loss_G 0.307\n",
            "[103/512][1750/1880] Loss_D: 1.213 Loss_G 2.067\n",
            "[103/512][1800/1880] Loss_D: 1.157 Loss_G 2.254\n",
            "[103/512][1850/1880] Loss_D: 1.499 Loss_G 2.072\n",
            "[104/512][0/1880] Loss_D: 1.599 Loss_G 1.349\n",
            "[104/512][50/1880] Loss_D: 1.294 Loss_G 2.166\n",
            "[104/512][100/1880] Loss_D: 1.248 Loss_G 1.183\n",
            "[104/512][150/1880] Loss_D: 1.351 Loss_G 2.108\n",
            "[104/512][200/1880] Loss_D: 1.321 Loss_G 1.024\n",
            "[104/512][250/1880] Loss_D: 1.331 Loss_G 2.418\n",
            "[104/512][300/1880] Loss_D: 1.185 Loss_G 2.193\n",
            "[104/512][350/1880] Loss_D: 1.482 Loss_G 2.181\n",
            "[104/512][400/1880] Loss_D: 1.419 Loss_G 0.437\n",
            "[104/512][450/1880] Loss_D: 1.324 Loss_G 2.003\n",
            "[104/512][500/1880] Loss_D: 1.236 Loss_G 1.565\n",
            "[104/512][550/1880] Loss_D: 1.472 Loss_G 0.384\n",
            "[104/512][600/1880] Loss_D: 1.279 Loss_G 2.013\n",
            "[104/512][650/1880] Loss_D: 1.264 Loss_G 1.156\n",
            "[104/512][700/1880] Loss_D: 1.774 Loss_G 1.439\n",
            "[104/512][750/1880] Loss_D: 1.209 Loss_G 0.571\n",
            "[104/512][800/1880] Loss_D: 1.452 Loss_G 1.696\n",
            "[104/512][850/1880] Loss_D: 1.524 Loss_G 1.838\n",
            "[104/512][900/1880] Loss_D: 1.269 Loss_G 1.797\n",
            "[104/512][950/1880] Loss_D: 1.261 Loss_G 2.405\n",
            "[104/512][1000/1880] Loss_D: 1.476 Loss_G 0.075\n",
            "[104/512][1050/1880] Loss_D: 1.287 Loss_G 0.164\n",
            "[104/512][1100/1880] Loss_D: 1.337 Loss_G 2.104\n",
            "[104/512][1150/1880] Loss_D: 1.300 Loss_G 0.969\n",
            "[104/512][1200/1880] Loss_D: 1.425 Loss_G 1.842\n",
            "[104/512][1250/1880] Loss_D: 1.360 Loss_G 1.198\n",
            "[104/512][1300/1880] Loss_D: 1.469 Loss_G 1.641\n",
            "[104/512][1350/1880] Loss_D: 1.251 Loss_G 0.574\n",
            "[104/512][1400/1880] Loss_D: 1.291 Loss_G 1.789\n",
            "[104/512][1450/1880] Loss_D: 1.196 Loss_G 2.617\n",
            "[104/512][1500/1880] Loss_D: 1.418 Loss_G 0.530\n",
            "[104/512][1550/1880] Loss_D: 1.498 Loss_G 1.769\n",
            "[104/512][1600/1880] Loss_D: 1.320 Loss_G 1.773\n",
            "[104/512][1650/1880] Loss_D: 1.273 Loss_G 1.095\n",
            "[104/512][1700/1880] Loss_D: 1.202 Loss_G 1.064\n",
            "[104/512][1750/1880] Loss_D: 1.216 Loss_G 1.644\n",
            "[104/512][1800/1880] Loss_D: 1.427 Loss_G 0.581\n",
            "[104/512][1850/1880] Loss_D: 1.245 Loss_G 2.024\n",
            "[105/512][0/1880] Loss_D: 1.496 Loss_G 1.719\n",
            "[105/512][50/1880] Loss_D: 1.550 Loss_G 1.790\n",
            "[105/512][100/1880] Loss_D: 1.362 Loss_G 0.868\n",
            "[105/512][150/1880] Loss_D: 1.235 Loss_G 2.224\n",
            "[105/512][200/1880] Loss_D: 1.549 Loss_G 1.950\n",
            "[105/512][250/1880] Loss_D: 1.366 Loss_G 1.978\n",
            "[105/512][300/1880] Loss_D: 1.259 Loss_G 2.070\n",
            "[105/512][350/1880] Loss_D: 1.721 Loss_G 1.309\n",
            "[105/512][400/1880] Loss_D: 1.297 Loss_G 1.361\n",
            "[105/512][450/1880] Loss_D: 1.222 Loss_G 1.400\n",
            "[105/512][500/1880] Loss_D: 1.358 Loss_G 2.207\n",
            "[105/512][550/1880] Loss_D: 1.431 Loss_G 1.966\n",
            "[105/512][600/1880] Loss_D: 1.479 Loss_G 0.271\n",
            "[105/512][650/1880] Loss_D: 1.289 Loss_G 0.240\n",
            "[105/512][700/1880] Loss_D: 1.332 Loss_G 1.079\n",
            "[105/512][750/1880] Loss_D: 1.306 Loss_G 0.606\n",
            "[105/512][800/1880] Loss_D: 1.455 Loss_G 0.639\n",
            "[105/512][850/1880] Loss_D: 1.245 Loss_G 0.763\n",
            "[105/512][900/1880] Loss_D: 1.277 Loss_G 2.343\n",
            "[105/512][950/1880] Loss_D: 1.436 Loss_G 0.618\n",
            "[105/512][1000/1880] Loss_D: 1.470 Loss_G 2.188\n",
            "[105/512][1050/1880] Loss_D: 1.412 Loss_G 1.391\n",
            "[105/512][1100/1880] Loss_D: 1.183 Loss_G 1.007\n",
            "[105/512][1150/1880] Loss_D: 1.369 Loss_G 2.053\n",
            "[105/512][1200/1880] Loss_D: 1.299 Loss_G 2.037\n",
            "[105/512][1250/1880] Loss_D: 1.655 Loss_G 1.471\n",
            "[105/512][1300/1880] Loss_D: 1.404 Loss_G 0.559\n",
            "[105/512][1350/1880] Loss_D: 1.514 Loss_G 1.154\n",
            "[105/512][1400/1880] Loss_D: 1.375 Loss_G 1.849\n",
            "[105/512][1450/1880] Loss_D: 1.542 Loss_G 1.323\n",
            "[105/512][1500/1880] Loss_D: 1.289 Loss_G 0.913\n",
            "[105/512][1550/1880] Loss_D: 1.213 Loss_G 0.348\n",
            "[105/512][1600/1880] Loss_D: 1.406 Loss_G 1.751\n",
            "[105/512][1650/1880] Loss_D: 1.382 Loss_G 2.532\n",
            "[105/512][1700/1880] Loss_D: 1.463 Loss_G 1.847\n",
            "[105/512][1750/1880] Loss_D: 1.283 Loss_G 0.945\n",
            "[105/512][1800/1880] Loss_D: 1.498 Loss_G 1.870\n",
            "[105/512][1850/1880] Loss_D: 1.281 Loss_G 1.951\n",
            "[106/512][0/1880] Loss_D: 1.358 Loss_G 1.369\n",
            "[106/512][50/1880] Loss_D: 1.300 Loss_G 1.398\n",
            "[106/512][100/1880] Loss_D: 1.343 Loss_G 0.548\n",
            "[106/512][150/1880] Loss_D: 1.298 Loss_G 0.677\n",
            "[106/512][200/1880] Loss_D: 1.460 Loss_G 1.420\n",
            "[106/512][250/1880] Loss_D: 1.288 Loss_G 0.532\n",
            "[106/512][300/1880] Loss_D: 1.343 Loss_G 1.732\n",
            "[106/512][350/1880] Loss_D: 1.246 Loss_G 0.317\n",
            "[106/512][400/1880] Loss_D: 1.266 Loss_G 0.707\n",
            "[106/512][450/1880] Loss_D: 1.104 Loss_G 0.859\n",
            "[106/512][500/1880] Loss_D: 1.328 Loss_G 2.304\n",
            "[106/512][550/1880] Loss_D: 1.607 Loss_G -0.108\n",
            "[106/512][600/1880] Loss_D: 1.198 Loss_G 0.590\n",
            "[106/512][650/1880] Loss_D: 1.516 Loss_G 1.875\n",
            "[106/512][700/1880] Loss_D: 1.315 Loss_G 0.417\n",
            "[106/512][750/1880] Loss_D: 1.299 Loss_G 0.716\n",
            "[106/512][800/1880] Loss_D: 1.298 Loss_G 0.711\n",
            "[106/512][850/1880] Loss_D: 1.290 Loss_G 0.258\n",
            "[106/512][900/1880] Loss_D: 1.261 Loss_G 0.551\n",
            "[106/512][950/1880] Loss_D: 1.298 Loss_G 1.901\n",
            "[106/512][1000/1880] Loss_D: 1.610 Loss_G 1.274\n",
            "[106/512][1050/1880] Loss_D: 1.393 Loss_G 1.856\n",
            "[106/512][1100/1880] Loss_D: 1.340 Loss_G 1.666\n",
            "[106/512][1150/1880] Loss_D: 1.223 Loss_G 1.905\n",
            "[106/512][1200/1880] Loss_D: 1.448 Loss_G 2.183\n",
            "[106/512][1250/1880] Loss_D: 1.428 Loss_G 0.224\n",
            "[106/512][1300/1880] Loss_D: 1.560 Loss_G 2.059\n",
            "[106/512][1350/1880] Loss_D: 1.460 Loss_G 2.392\n",
            "[106/512][1400/1880] Loss_D: 1.361 Loss_G 1.096\n",
            "[106/512][1450/1880] Loss_D: 1.413 Loss_G 1.925\n",
            "[106/512][1500/1880] Loss_D: 1.307 Loss_G 1.617\n",
            "[106/512][1550/1880] Loss_D: 1.521 Loss_G 2.111\n",
            "[106/512][1600/1880] Loss_D: 1.509 Loss_G 1.760\n",
            "[106/512][1650/1880] Loss_D: 1.331 Loss_G 0.356\n",
            "[106/512][1700/1880] Loss_D: 1.250 Loss_G 1.728\n",
            "[106/512][1750/1880] Loss_D: 1.274 Loss_G 1.768\n",
            "[106/512][1800/1880] Loss_D: 1.223 Loss_G 1.248\n",
            "[106/512][1850/1880] Loss_D: 1.335 Loss_G 2.109\n",
            "[107/512][0/1880] Loss_D: 1.286 Loss_G 1.032\n",
            "[107/512][50/1880] Loss_D: 1.415 Loss_G 0.547\n",
            "[107/512][100/1880] Loss_D: 1.255 Loss_G 1.972\n",
            "[107/512][150/1880] Loss_D: 1.389 Loss_G 0.713\n",
            "[107/512][200/1880] Loss_D: 1.171 Loss_G 1.305\n",
            "[107/512][250/1880] Loss_D: 1.313 Loss_G 1.061\n",
            "[107/512][300/1880] Loss_D: 1.300 Loss_G 1.806\n",
            "[107/512][350/1880] Loss_D: 1.265 Loss_G 1.261\n",
            "[107/512][400/1880] Loss_D: 1.154 Loss_G 0.752\n",
            "[107/512][450/1880] Loss_D: 1.266 Loss_G 1.989\n",
            "[107/512][500/1880] Loss_D: 1.105 Loss_G 0.817\n",
            "[107/512][550/1880] Loss_D: 1.456 Loss_G 0.370\n",
            "[107/512][600/1880] Loss_D: 1.197 Loss_G 1.076\n",
            "[107/512][650/1880] Loss_D: 1.482 Loss_G 1.301\n",
            "[107/512][700/1880] Loss_D: 1.251 Loss_G 0.846\n",
            "[107/512][750/1880] Loss_D: 1.152 Loss_G 1.421\n",
            "[107/512][800/1880] Loss_D: 1.597 Loss_G 2.194\n",
            "[107/512][850/1880] Loss_D: 1.528 Loss_G 0.046\n",
            "[107/512][900/1880] Loss_D: 1.379 Loss_G 2.228\n",
            "[107/512][950/1880] Loss_D: 1.555 Loss_G 2.111\n",
            "[107/512][1000/1880] Loss_D: 1.602 Loss_G 2.152\n",
            "[107/512][1050/1880] Loss_D: 1.277 Loss_G 0.852\n",
            "[107/512][1100/1880] Loss_D: 1.190 Loss_G 1.691\n",
            "[107/512][1150/1880] Loss_D: 1.190 Loss_G 1.049\n",
            "[107/512][1200/1880] Loss_D: 1.512 Loss_G 1.562\n",
            "[107/512][1250/1880] Loss_D: 1.218 Loss_G 2.448\n",
            "[107/512][1300/1880] Loss_D: 1.277 Loss_G 1.786\n",
            "[107/512][1350/1880] Loss_D: 1.349 Loss_G 0.970\n",
            "[107/512][1400/1880] Loss_D: 1.240 Loss_G 1.913\n",
            "[107/512][1450/1880] Loss_D: 1.395 Loss_G 1.750\n",
            "[107/512][1500/1880] Loss_D: 1.244 Loss_G 0.160\n",
            "[107/512][1550/1880] Loss_D: 1.306 Loss_G 0.565\n",
            "[107/512][1600/1880] Loss_D: 1.354 Loss_G 0.447\n",
            "[107/512][1650/1880] Loss_D: 1.342 Loss_G 1.745\n",
            "[107/512][1700/1880] Loss_D: 1.243 Loss_G 0.691\n",
            "[107/512][1750/1880] Loss_D: 1.343 Loss_G 1.910\n",
            "[107/512][1800/1880] Loss_D: 1.305 Loss_G 2.017\n",
            "[107/512][1850/1880] Loss_D: 1.383 Loss_G 0.399\n",
            "[108/512][0/1880] Loss_D: 1.427 Loss_G 1.735\n",
            "[108/512][50/1880] Loss_D: 1.376 Loss_G 1.811\n",
            "[108/512][100/1880] Loss_D: 1.256 Loss_G 2.324\n",
            "[108/512][150/1880] Loss_D: 1.519 Loss_G 1.824\n",
            "[108/512][200/1880] Loss_D: 1.451 Loss_G 2.138\n",
            "[108/512][250/1880] Loss_D: 1.368 Loss_G 0.856\n",
            "[108/512][300/1880] Loss_D: 1.384 Loss_G 0.451\n",
            "[108/512][350/1880] Loss_D: 1.293 Loss_G 1.745\n",
            "[108/512][400/1880] Loss_D: 1.538 Loss_G 1.788\n",
            "[108/512][450/1880] Loss_D: 1.195 Loss_G 1.469\n",
            "[108/512][500/1880] Loss_D: 1.384 Loss_G 1.993\n",
            "[108/512][550/1880] Loss_D: 1.416 Loss_G 1.897\n",
            "[108/512][600/1880] Loss_D: 1.698 Loss_G 1.380\n",
            "[108/512][650/1880] Loss_D: 1.373 Loss_G 1.043\n",
            "[108/512][700/1880] Loss_D: 1.438 Loss_G 0.864\n",
            "[108/512][750/1880] Loss_D: 1.337 Loss_G 1.820\n",
            "[108/512][800/1880] Loss_D: 1.238 Loss_G 1.512\n",
            "[108/512][850/1880] Loss_D: 1.279 Loss_G 1.690\n",
            "[108/512][900/1880] Loss_D: 1.509 Loss_G 0.678\n",
            "[108/512][950/1880] Loss_D: 1.606 Loss_G 2.078\n",
            "[108/512][1000/1880] Loss_D: 1.398 Loss_G 2.030\n",
            "[108/512][1050/1880] Loss_D: 1.483 Loss_G 1.781\n",
            "[108/512][1100/1880] Loss_D: 1.352 Loss_G 2.103\n",
            "[108/512][1150/1880] Loss_D: 1.291 Loss_G 2.010\n",
            "[108/512][1200/1880] Loss_D: 1.301 Loss_G 2.557\n",
            "[108/512][1250/1880] Loss_D: 1.326 Loss_G 0.364\n",
            "[108/512][1300/1880] Loss_D: 1.356 Loss_G 0.732\n",
            "[108/512][1350/1880] Loss_D: 1.516 Loss_G 1.727\n",
            "[108/512][1400/1880] Loss_D: 1.347 Loss_G 0.554\n",
            "[108/512][1450/1880] Loss_D: 1.300 Loss_G 1.598\n",
            "[108/512][1500/1880] Loss_D: 1.498 Loss_G 2.012\n",
            "[108/512][1550/1880] Loss_D: 1.040 Loss_G 1.522\n",
            "[108/512][1600/1880] Loss_D: 1.234 Loss_G 1.086\n",
            "[108/512][1650/1880] Loss_D: 1.348 Loss_G 0.974\n",
            "[108/512][1700/1880] Loss_D: 1.258 Loss_G 1.137\n",
            "[108/512][1750/1880] Loss_D: 1.051 Loss_G 2.336\n",
            "[108/512][1800/1880] Loss_D: 1.376 Loss_G 1.775\n",
            "[108/512][1850/1880] Loss_D: 1.076 Loss_G 1.941\n",
            "[109/512][0/1880] Loss_D: 1.206 Loss_G 2.539\n",
            "[109/512][50/1880] Loss_D: 1.114 Loss_G 2.446\n",
            "[109/512][100/1880] Loss_D: 1.128 Loss_G 2.624\n",
            "[109/512][150/1880] Loss_D: 1.190 Loss_G 0.228\n",
            "[109/512][200/1880] Loss_D: 1.093 Loss_G 1.629\n",
            "[109/512][250/1880] Loss_D: 1.208 Loss_G 2.189\n",
            "[109/512][300/1880] Loss_D: 1.177 Loss_G 2.639\n",
            "[109/512][350/1880] Loss_D: 1.437 Loss_G 1.982\n",
            "[109/512][400/1880] Loss_D: 1.293 Loss_G 2.357\n",
            "[109/512][450/1880] Loss_D: 1.132 Loss_G 1.169\n",
            "[109/512][500/1880] Loss_D: 1.308 Loss_G 2.526\n",
            "[109/512][550/1880] Loss_D: 1.120 Loss_G 2.615\n",
            "[109/512][600/1880] Loss_D: 1.236 Loss_G 2.275\n",
            "[109/512][650/1880] Loss_D: 1.165 Loss_G 1.123\n",
            "[109/512][700/1880] Loss_D: 1.189 Loss_G 2.031\n",
            "[109/512][750/1880] Loss_D: 1.292 Loss_G 0.743\n",
            "[109/512][800/1880] Loss_D: 1.122 Loss_G 2.198\n",
            "[109/512][850/1880] Loss_D: 1.252 Loss_G 0.663\n",
            "[109/512][900/1880] Loss_D: 1.234 Loss_G 1.005\n",
            "[109/512][950/1880] Loss_D: 1.182 Loss_G 2.557\n",
            "[109/512][1000/1880] Loss_D: 1.146 Loss_G 1.558\n",
            "[109/512][1050/1880] Loss_D: 1.223 Loss_G 2.277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2CjVIWZaF0t",
        "outputId": "eeabc570-a5bc-453a-88d0-6d8f8de417f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset[1][1][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2304,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx4w9KhOnxMu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}